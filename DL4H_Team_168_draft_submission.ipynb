{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "p7L2OMzrqhxs"
      ],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Before you use this template\n",
        "\n",
        "This template is just a recommended template for project Report. It only considers the general type of research in our paper pool. Feel free to edit it to better fit your project. You will iteratively update the same notebook submission for your draft and the final submission. Please check the project rubriks to get a sense of what is expected in the template.\n",
        "\n",
        "---\n",
        "\n",
        "# FAQ and Attentions\n",
        "* Copy and move this template to your Google Drive. Name your notebook by your team ID (upper-left corner). Don't eidt this original file.\n",
        "* This template covers most questions we want to ask about your reproduction experiment. You don't need to exactly follow the template, however, you should address the questions. Please feel free to customize your report accordingly.\n",
        "* any report must have run-able codes and necessary annotations (in text and code comments).\n",
        "* The notebook is like a demo and only uses small-size data (a subset of original data or processed data), the entire runtime of the notebook including data reading, data process, model training, printing, figure plotting, etc,\n",
        "must be within 8 min, otherwise, you may get penalty on the grade.\n",
        "  * If the raw dataset is too large to be loaded  you can select a subset of data and pre-process the data, then, upload the subset or processed data to Google Drive and load them in this notebook.\n",
        "  * If the whole training is too long to run, you can only set the number of training epoch to a small number, e.g., 3, just show that the training is runable.\n",
        "  * For results model validation, you can train the model outside this notebook in advance, then, load pretrained model and use it for validation (display the figures, print the metrics).\n",
        "* The post-process is important! For post-process of the results,please use plots/figures. The code to summarize results and plot figures may be tedious, however, it won't be waste of time since these figures can be used for presentation. While plotting in code, the figures should have titles or captions if necessary (e.g., title your figure with \"Figure 1. xxxx\")\n",
        "* There is not page limit to your notebook report, you can also use separate notebooks for the report, just make sure your grader can access and run/test them.\n",
        "* If you use outside resources, please refer them (in any formats). Include the links to the resources if necessary."
      ],
      "metadata": {
        "id": "j01aH0PR4Sg-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Notebook to Google Drive\n",
        "Upload the data, pretrianed model, figures, etc to your Google Drive, then mount this notebook to Google Drive. After that, you can access the resources freely.\n",
        "\n",
        "Instruction: https://colab.research.google.com/notebooks/io.ipynb\n",
        "\n",
        "Example: https://colab.research.google.com/drive/1srw_HFWQ2SMgmWIawucXfusGzrj1_U0q\n",
        "\n",
        "Video: https://www.youtube.com/watch?v=zc8g8lGcwQU"
      ],
      "metadata": {
        "id": "dlv6knX04FiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "sfk8Zrul_E8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "*Background*\n",
        "\n",
        "Surgery is often associated with fluctuations in blood pressure.(Salmasi V, Maheshwari K, Yang D, Mascha EJ, Singh A, Sessler DI, et al. Relationship between intraoperative hypotension, defined by either reduction from baseline or absolute thresholds, and acute kidney and myocardial injury after noncardiac surgery: a retrospective cohort analysis. Anesthesiology. 2017;126(1):47–65. pmid:27792044) Low blood pressure (‘intraoperative hypotension’) might occur due to medications administered (such as sedation) or blood loss. Hypotension is potentially dangerous as it might lead to reduced blood flow to vital organs such as the heart or the brain. Therefore, careful monitoring of the intraoperative blood pressure is performed in order to treat hypotensive events (usually defined as a mean arterial blood pressure [MAP] <65 mmHg) if they occur (commonly with fluids or medications). Continuously measured parameters such as the MAP, the patient’s ECG, or EEG might allow for earlier prediction of subsequent hypotensive events. This could, in turn, enable a more timely intervention and potentially even prevention of hypotensive events.\n",
        "\n",
        "\n",
        "*Paper explanation*\n",
        "\n",
        "Specific approach: This paper uses a public data repository of vital signs taken during surgery in 10 operating rooms at Seoul National University Hospital between 01/06/2005 and 03/01/2024. The final analysis included 14,140 patients undergoing non-cardiac surgery. Arterial blood pressure (ABP), Electrocardiogram (ECG), and Electroencephalogram (EEG) waveforms obtained during surgery were used to predict hypotensive events. Specifically, 1-min intervals of the waveforms were sampled 3, 5, 10, and 15 min before a hypotensive event (defined as a MAP<65 mmHg ≥1 min) and compared to waveforms prior to ‘non-events’ (samples in the middle of a 30 min window of a MAP ≥75 mmHg). Unreliable cases were removed using the J signal quality index (Li Q., Mark R.G. & Clifford G.D. Artificial arterial blood pressure artifact models and an evaluation of a robust blood pressure and heart rate estimator. BioMed Eng OnLine. 2009; 8(13). pmid:19586547).\n",
        "Following data preprocessing, the authors trained a ResNet CNN for each waveform. The outputs are subsequently concatenated and passed through a classifier to predict hypotensive events.\n"
      ],
      "metadata": {
        "id": "MQ0sNuMePBXx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scope of Reproducibility:\n",
        "\n",
        "*Hypothesis:*\n",
        "\n",
        "Is it possible to predict intraoperative hypotensive events using a deep-learning based analysis of MAP, ECG, and EEG waveforms?\n",
        "\n",
        "*Experimental setup:*\n",
        "\n",
        "Individual analysis of the predictive performance of the ABP, ECG, and EEG data compared to the model concatenating the results in order to understand the benefit vs the cost associated with the additional computation. Additional ablations will include studying the effect of the optimizer (Adam was used for the final model) and the learning rate (0.0001 in the paper).\n"
      ],
      "metadata": {
        "id": "uygL9tTPSVHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methodology\n"
      ],
      "metadata": {
        "id": "xWAHJ_1CdtaA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Link to github repo: https://github.com/sebbeyer/DLH_project_168.git"
      ],
      "metadata": {
        "id": "fbXThQzRLeAr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Install and load packages**"
      ],
      "metadata": {
        "id": "NbQQoc8TLhNv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vitaldb"
      ],
      "metadata": {
        "id": "Pt8hsfY3SJud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import  packages you need\n",
        "import os\n",
        "import sys\n",
        "import glob\n",
        "import math\n",
        "import copy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "import vitaldb\n",
        "import scipy.signal\n",
        "import scipy.io.wavfile"
      ],
      "metadata": {
        "id": "yu61Jp1xrnKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  **Load Data**\n",
        "\n",
        "*Source of the data:*\n",
        "\n",
        "The dataset used is open access: https://osf.io/dtc45/. It can be obtained after signing the Data Use Agreement (https://vitaldb.net/docs/?documentId=1OyhiDYbN-VJ6TOme-Fkj4wbqJkVT3UazELcbCXcHmiY)\n",
        "\n",
        "To run the notebook:\n",
        "1) Download the .vital files after signing the Data Use Agreement (each .vital file corresponds to the tracings of one patient during surgery)\n",
        "2) update 'raw_data_dir'\n",
        "\n",
        "Because of limited computing resources, this analysis will be limited to the first 2,000 patients included in the dataset."
      ],
      "metadata": {
        "id": "2NbPHUTMbkD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# show available tracks (all vital sign tracings obtained during surgery)\n",
        "tracks = vitaldb.vital_trks('/content/drive/MyDrive/VitalDB/vital_files_1_250/0001.vital')\n",
        "tracks"
      ],
      "metadata": {
        "id": "DFHNjHawcssd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_raw_data(raw_data_dir):\n",
        "\n",
        "  # load data to listsy with events and non-events\n",
        "  # generate separate lists for 3, 5, 10, and 15 min preceding events\n",
        "\n",
        "  ecg_3min = []\n",
        "  art_3min = []\n",
        "  eeg_3min = []\n",
        "  y_3min = []\n",
        "\n",
        "  ecg_5min = []\n",
        "  art_5min = []\n",
        "  eeg_5min = []\n",
        "  y_5min = []\n",
        "\n",
        "  ecg_10min = []\n",
        "  art_10min = []\n",
        "  eeg_10min = []\n",
        "  y_10min = []\n",
        "\n",
        "  ecg_15min = []\n",
        "  art_15min = []\n",
        "  eeg_15min = []\n",
        "  y_15min = []\n",
        "\n",
        "  # tracks to load\n",
        "  tracks = ['SNUADC/ECG_II', 'SNUADC/ART', 'BIS/EEG1_WAV'] # the paper does not specify which ECG lead they used for the analysis -> Here, we are using lead II\n",
        "  # the paper also does not specify which EEG waveform is being used -> Here, we are using waveform 1\n",
        "\n",
        "  # iterate over all files\n",
        "  for file in os.listdir(raw_data_dir):\n",
        "\n",
        "    #  extract ecg, blood pressure (art) and eeg data as numpy array\n",
        "    vital_object = vitaldb.VitalFile(file, tracks)\n",
        "    ecg_np = vital_object.to_numpy('SNUADC/ECG_II', 1/500)\n",
        "    art_np = vital_object.to_numpy('SNUADC/ART', 1/500)\n",
        "    eeg_np = vital_object.to_numpy('BIS/EEG1_WAV', 1/128)\n",
        "\n",
        "    # calculate mean arterial blood pressure (MAP) for 1 minute intervals\n",
        "    map = []\n",
        "\n",
        "    for i in range(0, art_np.shape[0], 30000): # sampling of 500Hz -> 30,000 datapoints / min\n",
        "      map.append(np.mean(art_np[i:i+30000]))\n",
        "\n",
        "    map = np.asarray(map)\n",
        "\n",
        "    # Quality check - exclude implausible values\n",
        "    map = np.where(map<20, np.nan, map)\n",
        "    map = np.where(map>200, np.nan, map)\n",
        "\n",
        "    # find index of events\n",
        "    # hypotensive events: MAP<65 mmHg\n",
        "\n",
        "    # index of hypotensive events mandating >20 min between each\n",
        "    # hypotensive event (the paper does not specify whether this is\n",
        "    # 20 min after the hypotension has completely resolved or 20 min\n",
        "    # after the onset of hypotension)\n",
        "    # -> here, I will require >20 min following the resolution of\n",
        "    # hypotension\n",
        "\n",
        "    # even more importantly, the authors did not specify how the following\n",
        "    # scenario should be handled: a second hypotensive event w/in 20 min\n",
        "    # of a prior hypotensive event with a third hypotensive event w/in 20 min\n",
        "    # of the second hypotensive event but >20 min after the first hypotensive\n",
        "    # event: should the third hypotensive event be considered an event and\n",
        "    # included in the analysis???\n",
        "    # -> here, I excluded even the third event as there are <20 min between\n",
        "    # consecutive hypotensive events\n",
        "\n",
        "    # the authors also didn't specify how they dealt with MAP>75 mmHg\n",
        "    # segemnts lasting >30 (i.e. when during those longer interavls they\n",
        "    # sampled controls)\n",
        "    # -> here, I will use the initial 30 min of such intervals\n",
        "\n",
        "    # 'events' array as indicator array:\n",
        "      # '0' : MAP 65 - 75 mmHg\n",
        "      # '1': MAP <65 mmHg with >20 min since the last hypotensive event\n",
        "      # '-1': MAP <65 mmHg with <= 20 min since last hypotensive event\n",
        "      # '2': MAP > 75 mmHg for 30 min (initial 30 min if MAP>75 for >30 min)\n",
        "\n",
        "    events = (map<65)*1 # MAP<65\n",
        "    events = np.where(np.isnan(map), np.nan, events) # keep nan as nan intervals\n",
        "\n",
        "    prec_20_min = np.asarray([np.nansum(events[max(i[0]-20, 0):i[0]]) for i in enumerate(events)]) # check for hypotensive events during 20 min interval preceding that timestamp\n",
        "    prec_20_min[prec_20_min >0] = 2\n",
        "\n",
        "    events = events - prec_20_min\n",
        "    events[events < -1] = 0\n",
        "\n",
        "    map_75 = (map>75)*2 # MAP>75\n",
        "    map_75 = np.where(np.isnan(map), np.nan, map_75) # keep nan as nan intervals\n",
        "\n",
        "    prec_30_min = np.asarray([np.nansum(map_75[max(i[0]-30, 0):i[0]]) for i in enumerate(map_75)]) # check for MAP>75 lasting >= 30 min\n",
        "    prec_30_min = (prec_30_min == 60).astype(int)\n",
        "\n",
        "    prec_30_min_2 = np.asarray([np.nansum(prec_30_min[max(i[0]-30, 0):i[0]]) for i in enumerate(prec_30_min)]) # identify initial 30 min w/ MAP >75 if longer time interval >75 mmHg\n",
        "    prec_30_min_2[prec_30_min_2 >0] = 2\n",
        "\n",
        "    map_75 = (map_75 * prec_30_min) - prec_30_min_2\n",
        "    map_75[map_75 == -2] = 0\n",
        "\n",
        "    events = events + map_75\n",
        "\n",
        "    # append lists\n",
        "\n",
        "    for i, j in enumerate(events):\n",
        "      if j == 1:\n",
        "        if (i >2 and np.isnan(ecg_np[(i-3)*30000:(i-2)*30000]).any() == False and\n",
        "        np.isnan(art_np[(i-3)*30000:(i-2)*30000]).any() == False and\n",
        "        np.isnan(eeg_np[(i-3)*7680:(i-2)*7680]).any() == False and\n",
        "        np.isnan(map[i-3]) == False):\n",
        "\n",
        "          ecg_3min.append(ecg_np[(i-3)*30000:(i-2)*30000])\n",
        "          art_3min.append(art_np[(i-3)*30000:(i-2)*30000])\n",
        "          eeg_3min.append(eeg_np[(i-3)*7680:(i-2)*7680])\n",
        "          y_3min.append(1)\n",
        "\n",
        "        if (i >4 and np.isnan(ecg_np[(i-5)*30000:(i-4)*30000]).any() == False and\n",
        "        np.isnan(art_np[(i-5)*30000:(i-4)*30000]).any() == False and\n",
        "        np.isnan(eeg_np[(i-5)*7680:(i-4)*7680]).any() == False and\n",
        "        np.isnan(map[i-5]) == False):\n",
        "\n",
        "          ecg_5min.append(ecg_np[(i-5)*30000:(i-4)*30000])\n",
        "          art_5min.append(art_np[(i-5)*30000:(i-4)*30000])\n",
        "          eeg_5min.append(eeg_np[(i-5)*7680:(i-4)*7680])\n",
        "          y_5min.append(1)\n",
        "\n",
        "        if (i >9 and np.isnan(ecg_np[(i-10)*30000:(i-9)*30000]).any() == False and\n",
        "        np.isnan(art_np[(i-10)*30000:(i-9)*30000]).any() == False and\n",
        "        np.isnan(eeg_np[(i-10)*7680:(i-9)*7680]).any() == False and\n",
        "        np.isnan(map[i-10]) == False):\n",
        "\n",
        "          ecg_10min.append(ecg_np[(i-10)*30000:(i-9)*30000])\n",
        "          art_10min.append(art_np[(i-10)*30000:(i-9)*30000])\n",
        "          eeg_10min.append(eeg_np[(i-10)*7680:(i-9)*7680])\n",
        "          y_10min.append(1)\n",
        "\n",
        "        if (i >14 and np.isnan(ecg_np[(i-15)*30000:(i-14)*30000]).any() == False and\n",
        "        np.isnan(art_np[(i-15)*30000:(i-14)*30000]).any() == False and\n",
        "        np.isnan(eeg_np[(i-15)*7680:(i-14)*7680]).any() == False and\n",
        "        np.isnan(map[i-15]) == False):\n",
        "\n",
        "          ecg_15min.append(ecg_np[(i-15)*30000:(i-14)*30000])\n",
        "          art_15min.append(art_np[(i-15)*30000:(i-14)*30000])\n",
        "          eeg_15min.append(eeg_np[(i-15)*7680:(i-14)*7680])\n",
        "          y_15min.append(1)\n",
        "\n",
        "      if (j == 2 and np.isnan(ecg_np[(i-15)*30000:(i-14)*30000]).any() == False and\n",
        "      np.isnan(art_np[(i-15)*30000:(i-14)*30000]).any() == False and\n",
        "      np.isnan(eeg_np[(i-15)*7680:(i-14)*7680]).any() == False and\n",
        "      np.isnan(map[i-15]) == False):\n",
        "\n",
        "        ecg_3min.append(ecg_np[(i-15)*30000:(i-14)*30000])\n",
        "        ecg_5min.append(ecg_np[(i-15)*30000:(i-14)*30000])\n",
        "        ecg_10min.append(ecg_np[(i-15)*30000:(i-14)*30000])\n",
        "        ecg_15min.append(ecg_np[(i-15)*30000:(i-14)*30000])\n",
        "        art_3min.append(art_np[(i-15)*30000:(i-14)*30000])\n",
        "        art_5min.append(art_np[(i-15)*30000:(i-14)*30000])\n",
        "        art_10min.append(art_np[(i-15)*30000:(i-14)*30000])\n",
        "        art_15min.append(art_np[(i-15)*30000:(i-14)*30000])\n",
        "        eeg_3min.append(eeg_np[(i-15)*7680:(i-14)*7680])\n",
        "        eeg_5min.append(eeg_np[(i-15)*7680:(i-14)*7680])\n",
        "        eeg_10min.append(eeg_np[(i-15)*7680:(i-14)*7680])\n",
        "        eeg_15min.append(eeg_np[(i-15)*7680:(i-14)*7680])\n",
        "        y_3min.append(0)\n",
        "        y_5min.append(0)\n",
        "        y_10min.append(0)\n",
        "        y_15min.append(0)\n",
        "\n",
        "  return (ecg_3min, art_3min, eeg_3min, y_3min, ecg_5min, art_5min, eeg_5min, y_5min,\n",
        "         ecg_10min, art_10min, eeg_10min, y_10min, ecg_15min, art_15min, eeg_15min, y_15min)\n"
      ],
      "metadata": {
        "id": "U7aDfa9_9GDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data_dir = ['/content/drive/MyDrive/VitalDB/vital_files_1_250',\n",
        "                '/content/drive/MyDrive/VitalDB/vital_files_251_500',\n",
        "                '/content/drive/MyDrive/VitalDB/vital_files_501_750',\n",
        "                '/content/drive/MyDrive/VitalDB/vital_files_751_1000',\n",
        "                '/content/drive/MyDrive/VitalDB/vital_files_1001_1250',\n",
        "                '/content/drive/MyDrive/VitalDB/vital_files_1251_1500',\n",
        "                '/content/drive/MyDrive/VitalDB/vital_files_1501_1750',\n",
        "                '/content/drive/MyDrive/VitalDB/vital_files_1751_2000']\n",
        "\n",
        "ecg_3min = []\n",
        "art_3min = []\n",
        "eeg_3min = []\n",
        "y_3min = []\n",
        "\n",
        "ecg_5min = []\n",
        "art_5min = []\n",
        "eeg_5min = []\n",
        "y_5min = []\n",
        "\n",
        "ecg_10min = []\n",
        "art_10min = []\n",
        "eeg_10min = []\n",
        "y_10min = []\n",
        "\n",
        "ecg_15min = []\n",
        "art_15min = []\n",
        "eeg_15min = []\n",
        "y_15min = []\n",
        "\n",
        "for i in raw_data_dir:\n",
        "\n",
        "  os.chdir(i)\n",
        "\n",
        "  j = load_raw_data(i)\n",
        "  ecg_3min.extend(j[0])\n",
        "  art_3min.extend(j[1])\n",
        "  eeg_3min.extend(j[2])\n",
        "  y_3min.extend(j[3])\n",
        "\n",
        "  ecg_5min.extend(j[4])\n",
        "  art_5min.extend(j[5])\n",
        "  eeg_5min.extend(j[6])\n",
        "  y_5min.extend(j[7])\n",
        "\n",
        "  ecg_10min.extend(j[8])\n",
        "  art_10min.extend(j[9])\n",
        "  eeg_10min.extend(j[10])\n",
        "  y_10min.extend(j[11])\n",
        "\n",
        "  ecg_15min.extend(j[12])\n",
        "  art_15min.extend(j[13])\n",
        "  eeg_15min.extend(j[14])\n",
        "  y_15min.extend(j[15])"
      ],
      "metadata": {
        "id": "TY_7F2sbAWl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to np array and save copy to google drive\n",
        "\n",
        "ecg_3min = np.asarray(ecg_3min)\n",
        "art_3min = np.asarray(art_3min)\n",
        "eeg_3min = np.asarray(eeg_3min)\n",
        "y_3min = np.asarray(y_3min)\n",
        "\n",
        "ecg_5min = np.asarray(ecg_5min)\n",
        "art_5min = np.asarray(art_5min)\n",
        "eeg_5min = np.asarray(eeg_5min)\n",
        "y_5min = np.asarray(y_5min)\n",
        "\n",
        "ecg_10min = np.asarray(ecg_10min)\n",
        "art_10min = np.asarray(art_10min)\n",
        "eeg_10min = np.asarray(eeg_10min)\n",
        "y_10min = np.asarray(y_10min)\n",
        "\n",
        "ecg_15min = np.asarray(ecg_15min)\n",
        "art_15min = np.asarray(art_15min)\n",
        "eeg_15min = np.asarray(eeg_15min)\n",
        "y_15min = np.asarray(y_15min)\n",
        "\n",
        "os.chdir('/content/drive/MyDrive/VitalDB')\n",
        "\n",
        "np.save('/content/drive/MyDrive/VitalDB/ecg_3min2.npy', ecg_3min)\n",
        "np.save('/content/drive/MyDrive/VitalDB/art_3min2.npy', art_3min)\n",
        "np.save('/content/drive/MyDrive/VitalDB/eeg_3min2.npy', eeg_3min)\n",
        "np.save('/content/drive/MyDrive/VitalDB/y_3min2.npy', y_3min)\n",
        "\n",
        "np.save('/content/drive/MyDrive/VitalDB/ecg_5min2.npy', ecg_5min)\n",
        "np.save('/content/drive/MyDrive/VitalDB/art_5min2.npy', art_5min)\n",
        "np.save('/content/drive/MyDrive/VitalDB/eeg_5min2.npy', eeg_5min)\n",
        "np.save('/content/drive/MyDrive/VitalDB/y_5min2.npy', y_5min)\n",
        "\n",
        "np.save('/content/drive/MyDrive/VitalDB/ecg_10min2.npy', ecg_10min)\n",
        "np.save('/content/drive/MyDrive/VitalDB/art_10min2.npy', art_10min)\n",
        "np.save('/content/drive/MyDrive/VitalDB/eeg_10min2.npy', eeg_10min)\n",
        "np.save('/content/drive/MyDrive/VitalDB/y_10min2.npy', y_10min)\n",
        "\n",
        "np.save('/content/drive/MyDrive/VitalDB/ecg_15min2.npy', ecg_15min)\n",
        "np.save('/content/drive/MyDrive/VitalDB/art_15min2.npy', art_15min)\n",
        "np.save('/content/drive/MyDrive/VitalDB/eeg_15min2.npy', eeg_15min)\n",
        "np.save('/content/drive/MyDrive/VitalDB/y_15min2.npy', y_15min)"
      ],
      "metadata": {
        "id": "GnNpuazTDA-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load copies\n",
        "ecg_3min = np.load('/content/drive/MyDrive/VitalDB/ecg_3min.npy')\n",
        "art_3min = np.load('/content/drive/MyDrive/VitalDB/art_3min.npy')\n",
        "eeg_3min = np.load('/content/drive/MyDrive/VitalDB/eeg_3min.npy')\n",
        "y_3min = np.load('/content/drive/MyDrive/VitalDB/y_3min.npy')\n",
        "\n",
        "ecg_5min = np.load('/content/drive/MyDrive/VitalDB/ecg_5min.npy')\n",
        "art_5min = np.load('/content/drive/MyDrive/VitalDB/art_5min.npy')\n",
        "eeg_5min = np.load('/content/drive/MyDrive/VitalDB/eeg_5min.npy')\n",
        "y_5min = np.load('/content/drive/MyDrive/VitalDB/y_5min.npy')\n",
        "\n",
        "ecg_10min = np.load('/content/drive/MyDrive/VitalDB/ecg_10min.npy')\n",
        "art_10min = np.load('/content/drive/MyDrive/VitalDB/art_10min.npy')\n",
        "eeg_10min = np.load('/content/drive/MyDrive/VitalDB/eeg_10min.npy')\n",
        "y_10min = np.load('/content/drive/MyDrive/VitalDB/y_10min.npy')\n",
        "\n",
        "ecg_15min = np.load('/content/drive/MyDrive/VitalDB/ecg_15min.npy')\n",
        "art_15min = np.load('/content/drive/MyDrive/VitalDB/art_15min.npy')\n",
        "eeg_15min = np.load('/content/drive/MyDrive/VitalDB/eeg_15min.npy')\n",
        "y_15min = np.load('/content/drive/MyDrive/VitalDB/y_15min.npy')"
      ],
      "metadata": {
        "id": "gdIezgeVRZm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Preprocessing**"
      ],
      "metadata": {
        "id": "eR5F1Ej-L0vH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply frequency filter\n",
        "# While the frequencies are provided in the paper, additional technical\n",
        "# details such as the type of filter or the filter settings are not mentioned\n",
        "# -> here, I am using a 4-th order Butterworth filter\n",
        "\n",
        "def bandpass(data, edges, sampling_rate, poles: int = 4):\n",
        "    sos = scipy.signal.butter(poles, edges, 'bandpass', fs=sampling_rate, output='sos')\n",
        "    filtered_data = scipy.signal.sosfiltfilt(sos, data, axis=1)\n",
        "    return filtered_data"
      ],
      "metadata": {
        "id": "umpkR4Ipp0OY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing ECGs using Z scores\n",
        "# It is not clear from the paper whether Z score are calculated for each sample\n",
        "# or for the entire dataset\n",
        "\n",
        "def normalize(data):\n",
        "\n",
        "  mean_data = np.mean(data)\n",
        "  sd_data = np.std(data)\n",
        "  normalized_data = (data - mean_data) / sd_data\n",
        "  return normalized_data"
      ],
      "metadata": {
        "id": "_QAuFWNQsTjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# process raw data\n",
        "sampling_rate_ecg = 500\n",
        "fmin_ecg = 1\n",
        "fmax_ecg = 40\n",
        "\n",
        "sampling_rate_eeg = 128\n",
        "fmin_eeg = 0.5\n",
        "fmax_eeg = 50\n",
        "\n",
        "def process_data(ecg_3min, ecg_5min, ecg_10min, ecg_15min, eeg_3min, eeg_5min, eeg_10min, eeg_15min,\n",
        "                 sampling_rate_ecg, fmin_ecg, fmax_ecg, sampling_rate_eeg, fmin_eeg, fmax_eeg):\n",
        "\n",
        "  # bandpass filter\n",
        "  ecg_3min_filtered = bandpass(ecg_3min, [fmin_ecg, fmax_ecg], sampling_rate_ecg)\n",
        "  ecg_5min_filtered = bandpass(ecg_5min, [fmin_ecg, fmax_ecg], sampling_rate_ecg)\n",
        "  ecg_10min_filtered = bandpass(ecg_10min, [fmin_ecg, fmax_ecg], sampling_rate_ecg)\n",
        "  ecg_15min_filtered = bandpass(ecg_15min, [fmin_ecg, fmax_ecg], sampling_rate_ecg)\n",
        "\n",
        "  eeg_3min_filtered = bandpass(eeg_3min, [fmin_eeg, fmax_eeg], sampling_rate_eeg)\n",
        "  eeg_5min_filtered = bandpass(eeg_5min, [fmin_eeg, fmax_eeg], sampling_rate_eeg)\n",
        "  eeg_10min_filtered = bandpass(eeg_10min, [fmin_eeg, fmax_eeg], sampling_rate_eeg)\n",
        "  eeg_15min_filtered = bandpass(eeg_15min, [fmin_eeg, fmax_eeg], sampling_rate_eeg)\n",
        "\n",
        "  # normalizing ECG using Z score\n",
        "  ecg_3min_normalized = normalize(ecg_3min_filtered)\n",
        "  ecg_5min_normalized = normalize(ecg_5min_filtered)\n",
        "  ecg_10min_normalized = normalize(ecg_10min_filtered)\n",
        "  ecg_15min_normalized = normalize(ecg_15min_filtered)\n",
        "\n",
        "  return ecg_3min_normalized, ecg_5min_normalized, ecg_10min_normalized, ecg_15min_normalized, \\\n",
        "        eeg_3min_filtered, eeg_5min_filtered, eeg_10min_filtered, eeg_15min_filtered\n",
        "\n",
        "ecg_3min_normalized, ecg_5min_normalized, ecg_10min_normalized, ecg_15min_normalized, \\\n",
        "eeg_3min_filtered, eeg_5min_filtered, eeg_10min_filtered, eeg_15min_filtered = \\\n",
        "process_data(ecg_3min, ecg_5min, ecg_10min, ecg_15min, eeg_3min, eeg_5min, eeg_10min, eeg_15min,\n",
        "                 sampling_rate_ecg, fmin_ecg, fmax_ecg, sampling_rate_eeg, fmin_eeg, fmax_eeg)"
      ],
      "metadata": {
        "id": "A6fJPhBvS9Gp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Statistics and Sample Tracings**"
      ],
      "metadata": {
        "id": "A9FqYwGDL9BO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Statistics\n",
        "\n",
        "# calculate statistics\n",
        "def calculate_stats(y):\n",
        "\n",
        "  n_samples_3min = len(y)\n",
        "  cases_3min = np.sum(y)\n",
        "  controls_3min = len(y) - np.sum(y)\n",
        "\n",
        "  return n_samples_3min, cases_3min, controls_3min\n",
        "\n",
        "n_samples_3min, cases_3min, controls_3min = calculate_stats(y_3min)\n",
        "\n",
        "print(f'total number of samples with at least 3 minutes of data prior to event: {n_samples_3min}')\n",
        "print(f'total number of cases with at least 3 minutes of data prior to event: {cases_3min}')\n",
        "print(f'total number of controls: {controls_3min}')"
      ],
      "metadata": {
        "id": "oOf4YGOYaf21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot sample waveforms\n",
        "\n",
        "ecg = ecg_3min_normalized[0,:]\n",
        "art = art_3min[0,:]\n",
        "eeg = eeg_3min_filtered[0,:]\n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.subplot(311)\n",
        "plt.plot(ecg[0:1000], color='g')\n",
        "plt.subplot(312)\n",
        "plt.plot(art[0:1000], color='r')\n",
        "plt.subplot(313)\n",
        "plt.plot(eeg[0:1000], color='b')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Qm8oagw3UARL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Train / Test Split and Dataset / Dataloader**"
      ],
      "metadata": {
        "id": "pi6TKR7VMDMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training and test samples (80:20 split)\n",
        "# Since each patient might contribute multiple samples, shuffling will not be used at this point\n",
        "\n",
        "def split_train_test(ecg, art, eeg, y, ratio):\n",
        "\n",
        "  split = int(ratio * len(y))\n",
        "\n",
        "  ecg_train = ecg[:split]\n",
        "  ecg_test = ecg[split:]\n",
        "  art_train = art[:split]\n",
        "  art_test = art[split:]\n",
        "  eeg_train = eeg[:split]\n",
        "  eeg_test = eeg[split:]\n",
        "  y_train = y[:split]\n",
        "  y_test = y[split:]\n",
        "\n",
        "  return ecg_train, ecg_test, art_train, art_test, eeg_train, eeg_test, y_train, y_test\n",
        "\n",
        "ecg_3min_train, ecg_3min_test, art_3min_train, art_3min_test, eeg_3min_train, eeg_3min_test, y_3min_train, y_3min_test = \\\n",
        "split_train_test(ecg_3min_normalized, art_3min, eeg_3min_filtered, y_3min, 0.8)\n",
        "\n",
        "ecg_5min_train, ecg_5min_test, art_5min_train, art_5min_test, eeg_5min_train, eeg_5min_test, y_5min_train, y_5min_test = \\\n",
        "split_train_test(ecg_5min_normalized, art_5min, eeg_5min_filtered, y_5min, 0.8)\n",
        "\n",
        "ecg_10min_train, ecg_10min_test, art_10min_train, art_10min_test, eeg_10min_train, eeg_10min_test, y_10min_train, y_10min_test = \\\n",
        "split_train_test(ecg_10min_normalized, art_10min, eeg_10min_filtered, y_10min, 0.8)\n",
        "\n",
        "ecg_15min_train, ecg_15min_test, art_15min_train, art_15min_test, eeg_15min_train, eeg_15min_test, y_15min_train, y_15min_test = \\\n",
        "split_train_test(ecg_15min_normalized, art_15min, eeg_15min_filtered, y_15min, 0.8)"
      ],
      "metadata": {
        "id": "nhqrqezcbUht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Custom Dataset class\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class HypoDataset(Dataset):\n",
        "\n",
        "    def __init__(self, ecg, art, eeg, y):\n",
        "\n",
        "        super().__init__()\n",
        "        self.y = y\n",
        "        self.ecg = ecg\n",
        "        self.art = art\n",
        "        self.eeg = eeg\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "\n",
        "        return ((self.ecg[i], self.art[i], self.eeg[i]), self.y[i])"
      ],
      "metadata": {
        "id": "P_-hk9rpRDLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to load dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def load_data(dataset, batch_size=32):\n",
        "    \"\"\"\n",
        "    Return a DataLoader instance basing on a Dataset instance, with batch_size specified.\n",
        "    \"\"\"\n",
        "    def my_collate(batch):\n",
        "\n",
        "        # your code here\n",
        "        x, y = zip(*batch)\n",
        "        ecg, art, eeg = zip(*x)\n",
        "\n",
        "        Y = torch.tensor(y, dtype=torch.float)\n",
        "\n",
        "        ECG = torch.tensor(ecg, dtype=torch.float).transpose(1,2)\n",
        "        ART = torch.tensor(art, dtype=torch.float).transpose(1,2)\n",
        "        EEG = torch.tensor(eeg, dtype=torch.float).transpose(1,2)\n",
        "\n",
        "        return (ECG, ART, EEG), Y\n",
        "\n",
        "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=my_collate)"
      ],
      "metadata": {
        "id": "QfxjIvPV__Ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader_3min = load_data(HypoDataset(ecg_3min_train, art_3min_train, eeg_3min_train, y_3min_train))\n",
        "test_loader_3min = load_data(HypoDataset(ecg_3min_test, art_3min_test, eeg_3min_test, y_3min_test))\n",
        "\n",
        "train_loader_5min = load_data(HypoDataset(ecg_5min_train, art_5min_train, eeg_5min_train, y_5min_train))\n",
        "test_loader_5min = load_data(HypoDataset(ecg_5min_test, art_5min_test, eeg_5min_test, y_5min_test))\n",
        "\n",
        "train_loader_10min = load_data(HypoDataset(ecg_10min_train, art_10min_train, eeg_10min_train, y_10min_train))\n",
        "test_loader_10min = load_data(HypoDataset(ecg_10min_test, art_10min_test, eeg_10min_test, y_10min_test))\n",
        "\n",
        "train_loader_15min = load_data(HypoDataset(ecg_15min_train, art_15min_train, eeg_15min_train, y_15min_train))\n",
        "test_loader_15min = load_data(HypoDataset(ecg_15min_test, art_15min_test, eeg_15min_test, y_15min_test))"
      ],
      "metadata": {
        "id": "WW_cwWQjDiBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Architecture**"
      ],
      "metadata": {
        "id": "p7L2OMzrqhxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# the model architecture and details are poorly described and\n",
        "# discrepant. quite frankly, this makes me question the choice of\n",
        "# this paper as an option for a final project in this class:\n",
        "\n",
        "# 1)\n",
        "# the text mentions an additional encoder block (conv + dropout)\n",
        "# (before the data are passed through the residual blocks), which\n",
        "# is not shown in Fig 2 or Suppl. Table 1. Even more concerning, in the\n",
        "# text it says that the encoder blocks consist of a conv layer and a\n",
        "# max pooling layer whose technical specifications aren't mentioned\n",
        "# at all\n",
        "\n",
        "# 2)\n",
        "# at least some of the conv layers have to use padding since residual\n",
        "# connections are being used. However, padding is not mentioned at all\n",
        "\n",
        "# 3)\n",
        "# Supple Table 1 (detailing the hyperparameter settings) is not\n",
        "# consistent with the description of the model in the text of Fig 2:\n",
        "# the output size in Suppl Table 1 implies pooling layers are being\n",
        "# used between every other residual layer, but this is inconsistent\n",
        "# with the text or figure\n",
        "\n",
        "# 4)\n",
        "# According to Suppl. Table 1, channels increases w/ subsequent residual blocks,\n",
        "# but according to Fig 2 each layer has to conv layers - ???which layer\n",
        "# increases the channels???\n",
        "\n",
        "# 5)\n",
        "# According to Suppl. Table 1, kernel sizes change with subsequent\n",
        "# residual blocks - this is contradictory to what is mentioned in the text\n",
        "\n",
        "# 6)\n",
        "# What kind of activation function do the linear layers have? Relu???\n",
        "\n",
        "# 7)\n",
        "# Some residual blocks increase the number of channels -> a residual\n",
        "# connection is not possible here (unless the number of channels of the\n",
        "# input is also being adjusted for the skip connection, which is\n",
        "# also not mentioned at all in the text)\n",
        "\n",
        "# 8)\n",
        "# The output size numbers provided in Suppl Table 1 don't add up:\n",
        "# Max pooling w/ (2,2) applied to 1875 results in 937, not 938\n",
        "# Max pooling w/ (2,2) applied to 937 results in 468 (and even\n",
        "# max pooling applied to 938 does not result in 496)\n",
        "\n",
        "# 9)\n",
        "# it is unclear how weights were initialized\n",
        "\n",
        "# 10)\n",
        "# The authors do not mention the software / package they used for the analysis!\n",
        "\n",
        "class my_model(nn.Module):\n",
        "  # use this class to define your model\n",
        "  def __init__(self):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder_ecg = nn.Sequential(\n",
        "                        nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer1_ecg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(1),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(1,2,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_ecg_layer1 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer2_ecg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer3_ecg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_ecg_layer3 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer4_ecg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer5_ecg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_ecg_layer5 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer6_ecg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,4,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(4,4,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer7_ecg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(4,4,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(4,4,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_ecg_layer7 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer8_ecg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(4,4,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(4,4,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer9_ecg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(4,4,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(4,4,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_ecg_layer9 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer10_ecg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(4,6,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(6,6,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer11_ecg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(6,6,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(6,6,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_ecg_layer11 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer12_ecg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(6,6,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(6,6,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.linear_ecg = nn.Linear(468*6, 32)\n",
        "\n",
        "    self.encoder_art = nn.Sequential(\n",
        "                        nn.Conv1d(1,1,3,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer1_art = nn.Sequential(\n",
        "                        nn.BatchNorm1d(1),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(1,2,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_art_layer1 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer2_art = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer3_art = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_art_layer3 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer4_art = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer5_art = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_art_layer5 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer6_art = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,4,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(4,4,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer7_art = nn.Sequential(\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(4,4,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(4,4,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_art_layer7 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer8_art = nn.Sequential(\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(4,4,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(4,4,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer9_art = nn.Sequential(\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(4,4,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(4,4,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_art_layer9 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer10_art = nn.Sequential(\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(4,6,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(6,6,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer11_art = nn.Sequential(\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(6,6,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(6,6,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_art_layer11 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer12_art = nn.Sequential(\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(6,6,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(6,6,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.linear_art = nn.Linear(468*6, 32)\n",
        "\n",
        "    self.encoder_eeg = nn.Sequential(\n",
        "                        nn.Conv1d(1,1,3,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer1_eeg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(1),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(1,2,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_eeg_layer1 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer2_eeg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer3_eeg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_eeg_layer3 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer4_eeg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer5_eeg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_eeg_layer5 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer6_eeg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,4,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(4,4,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer7_eeg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(4,4,3,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(4,4,3,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_eeg_layer7 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer8_eeg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(4,4,3,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(4,4,3,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer9_eeg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(4,4,3,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(4,4,3,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_eeg_layer9 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer10_eeg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(4,6,3,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(6,6,3,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer11_eeg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(6,6,3,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(6,6,3,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_eeg_layer11 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer12_eeg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(6,6,3,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(6,6,3,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.linear_eeg = nn.Linear(120*6, 32)\n",
        "\n",
        "    self.linear_combined1 = nn.Linear(96, 16)\n",
        "    self.linear_combined2 = nn.Linear(16, 1)\n",
        "\n",
        "  def forward(self, ecg, art, eeg):\n",
        "\n",
        "    ecg = self.encoder_ecg(ecg)\n",
        "    tmp = self.layer1_ecg(ecg)\n",
        "    ecg = tmp + ecg\n",
        "    ecg = self.maxpool_ecg_layer1(ecg)\n",
        "    tmp = self.layer2_ecg(ecg)\n",
        "    ecg = tmp + ecg\n",
        "    tmp = self.layer3_ecg(ecg)\n",
        "    ecg = tmp + ecg\n",
        "    ecg = self.maxpool_ecg_layer3(ecg)\n",
        "    tmp = self.layer4_ecg(ecg)\n",
        "    ecg = tmp + ecg\n",
        "    tmp = self.layer5_ecg(ecg)\n",
        "    ecg = tmp + ecg\n",
        "    ecg = self.maxpool_ecg_layer5(ecg)\n",
        "    ecg = self.layer6_ecg(ecg)\n",
        "    tmp = self.layer7_ecg(ecg)\n",
        "    ecg = tmp + ecg\n",
        "    ecg = self.maxpool_ecg_layer7(ecg)\n",
        "    tmp = self.layer8_ecg(ecg)\n",
        "    ecg = tmp + ecg\n",
        "    tmp = self.layer9_ecg(ecg)\n",
        "    ecg = tmp + ecg\n",
        "    ecg = self.maxpool_ecg_layer9(ecg)\n",
        "    ecg = self.layer10_ecg(ecg)\n",
        "    tmp = self.layer11_ecg(ecg)\n",
        "    ecg = tmp + ecg\n",
        "    ecg = self.maxpool_ecg_layer11(ecg)\n",
        "    tmp = self.layer12_ecg(ecg)\n",
        "    ecg = tmp + ecg\n",
        "    ecg = torch.flatten(ecg, 1)\n",
        "    ecg = F.relu(self.linear_ecg(ecg))\n",
        "\n",
        "    art = self.encoder_art(art)\n",
        "    tmp = self.layer1_art(art)\n",
        "    art = tmp + art\n",
        "    art = self.maxpool_art_layer1(art)\n",
        "    tmp = self.layer2_art(art)\n",
        "    art = tmp + art\n",
        "    tmp = self.layer3_art(art)\n",
        "    art = tmp + art\n",
        "    art = self.maxpool_art_layer3(art)\n",
        "    tmp = self.layer4_art(art)\n",
        "    art = tmp + art\n",
        "    tmp = self.layer5_art(art)\n",
        "    art = tmp + art\n",
        "    art = self.maxpool_art_layer5(art)\n",
        "    art = self.layer6_art(art)\n",
        "    tmp = self.layer7_art(art)\n",
        "    art = tmp + art\n",
        "    art = self.maxpool_art_layer7(art)\n",
        "    tmp = self.layer8_art(art)\n",
        "    art = tmp + art\n",
        "    tmp = self.layer9_art(art)\n",
        "    art = tmp + art\n",
        "    art = self.maxpool_art_layer9(art)\n",
        "    art = self.layer10_art(art)\n",
        "    tmp = self.layer11_art(art)\n",
        "    art = tmp + art\n",
        "    art = self.maxpool_art_layer11(art)\n",
        "    tmp = self.layer12_art(art)\n",
        "    art = tmp + art\n",
        "    art = torch.flatten(art, 1)\n",
        "    art = F.relu(self.linear_art(art))\n",
        "\n",
        "    eeg = self.encoder_eeg(eeg)\n",
        "    tmp = self.layer1_eeg(eeg)\n",
        "    eeg = tmp + eeg\n",
        "    eeg = self.maxpool_eeg_layer1(eeg)\n",
        "    tmp = self.layer2_eeg(eeg)\n",
        "    eeg = tmp + eeg\n",
        "    tmp = self.layer3_eeg(eeg)\n",
        "    eeg = tmp + eeg\n",
        "    eeg = self.maxpool_eeg_layer3(eeg)\n",
        "    tmp = self.layer4_eeg(eeg)\n",
        "    eeg = tmp + eeg\n",
        "    tmp = self.layer5_eeg(eeg)\n",
        "    eeg = tmp + eeg\n",
        "    eeg = self.maxpool_eeg_layer5(eeg)\n",
        "    eeg = self.layer6_eeg(eeg)\n",
        "    tmp = self.layer7_eeg(eeg)\n",
        "    eeg = tmp + eeg\n",
        "    eeg = self.maxpool_eeg_layer7(eeg)\n",
        "    tmp = self.layer8_eeg(eeg)\n",
        "    eeg = tmp + eeg\n",
        "    tmp = self.layer9_eeg(eeg)\n",
        "    eeg = tmp + eeg\n",
        "    eeg = self.maxpool_eeg_layer9(eeg)\n",
        "    eeg = self.layer10_eeg(eeg)\n",
        "    tmp = self.layer11_eeg(eeg)\n",
        "    eeg = tmp + eeg\n",
        "    eeg = self.maxpool_eeg_layer11(eeg)\n",
        "    tmp = self.layer12_eeg(eeg)\n",
        "    eeg = tmp + eeg\n",
        "    eeg = torch.flatten(eeg, 1)\n",
        "    eeg = F.relu(self.linear_eeg(eeg))\n",
        "\n",
        "    combined = F.relu(self.linear_combined1(torch.cat((ecg, art, eeg), -1)))\n",
        "    logits = self.linear_combined2(combined)\n",
        "    # probs = F.sigmoid(logits)\n",
        "\n",
        "    return logits\n",
        "\n",
        "model = my_model()"
      ],
      "metadata": {
        "id": "gBdVZoTvsSFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training**"
      ],
      "metadata": {
        "id": "q0UrnXzPqqim"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this draft, training will be limited to the 3 minute data"
      ],
      "metadata": {
        "id": "9Qdm3uahM724"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_one_iter(train_dataloader, model, loss_func, optimizer):\n",
        "\n",
        "  model.train()\n",
        "  running_loss = 0\n",
        "\n",
        "  for (ecg, art, eeg), y in train_dataloader:\n",
        "\n",
        "    logits = model(ecg, art, eeg)\n",
        "    loss = loss_func(logits.view(logits.shape[0]), y)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "\n",
        "  epoch_loss = running_loss / len(train_dataloader)\n",
        "\n",
        "  return model, epoch_loss"
      ],
      "metadata": {
        "id": "K7AyEc_NqeuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_func = nn.BCEWithLogitsLoss()\n",
        "lr = 0.001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "\n",
        "num_epoch = 10\n",
        "\n",
        "# model training loop: it is better to print the training/validation losses during the training\n",
        "\n",
        "# 3 min model\n",
        "for i in range(num_epoch):\n",
        "\n",
        "  model, train_epoch_loss = train_model_one_iter(train_loader_3min, model, loss_func, optimizer)\n",
        "  print(f'Epoch: {i}, Train loss: {train_epoch_loss}')\n"
      ],
      "metadata": {
        "id": "QPJJjwCpqDY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creat general checkpoint\n",
        "\n",
        "# 3 min model\n",
        "EPOCH = 9\n",
        "PATH = \"/content/drive/MyDrive/VitalDB/model_3min.pt\"\n",
        "LOSS = 0.37147167262057185\n",
        "\n",
        "torch.save({\n",
        "            'epoch': EPOCH,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': LOSS,\n",
        "            }, PATH)"
      ],
      "metadata": {
        "id": "7UXlBf8rKBgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "In this section, you should finish training your model training or loading your trained model. That is a great experiment! You should share the results with others with necessary metrics and figures.\n",
        "\n",
        "Please test and report results for all experiments that you run with:\n",
        "\n",
        "*   specific numbers (accuracy, AUC, RMSE, etc)\n",
        "*   figures (loss shrinkage, outputs from GAN, annotation or label of sample pictures, etc)\n"
      ],
      "metadata": {
        "id": "gX6bCcZNuxmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the general checkpoint\n",
        "\n",
        "# 3 min model\n",
        "model = my_model()\n",
        "lr = 0.001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "\n",
        "PATH = \"/content/drive/MyDrive/VitalDB/model_3min.pt\"\n",
        "\n",
        "checkpoint = torch.load(PATH)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "epoch = checkpoint['epoch']\n",
        "loss = checkpoint['loss']"
      ],
      "metadata": {
        "id": "kpL50BJZNJ_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Evaluation**"
      ],
      "metadata": {
        "id": "iHsT7oNgq1Ij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model(test_dataloader, model):\n",
        "\n",
        "    model.eval()\n",
        "    Y_score = []\n",
        "    Y_pred = []\n",
        "    Y_true = []\n",
        "\n",
        "    for (ecg, art, eeg), y in test_dataloader:\n",
        "\n",
        "        y_hat = torch.sigmoid(model(ecg, art, eeg))\n",
        "        y_hat = y_hat.detach()\n",
        "        Y_score.append(y_hat)\n",
        "\n",
        "        y_hat = (y_hat>0.5).int()\n",
        "        Y_pred.append(y_hat)\n",
        "\n",
        "        Y_true.append(y)\n",
        "\n",
        "    Y_score = np.concatenate(Y_score, axis=0)\n",
        "    Y_pred = np.concatenate(Y_pred, axis=0)\n",
        "    Y_true = np.concatenate(Y_true, axis=0)\n",
        "\n",
        "    return Y_score, Y_pred, Y_true"
      ],
      "metadata": {
        "id": "jqKj15x4-dUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_score, y_pred, y_true = eval_model(test_loader_3min, model)"
      ],
      "metadata": {
        "id": "_2JXiZBgBH-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "\n",
        "acc = accuracy_score(y_true, y_pred)\n",
        "roc_auc = roc_auc_score(y_true, y_score)\n",
        "\n",
        "print(f'Test Accuracy: {acc}')\n",
        "print(f'Test AUC: {roc_auc}')"
      ],
      "metadata": {
        "id": "VD4HmYpZAcME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy is currently significantly lower than in the original paper, likely because i) I only used 2,000 patients and ii) there are many details the authors don't comment on (see comments above) which may have led to this model being different from the one they used"
      ],
      "metadata": {
        "id": "r_njFmv9U6nh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# metrics to evaluate my model\n",
        "\n",
        "# plot figures to better show the results\n",
        "\n",
        "# it is better to save the numbers and figures for your presentation."
      ],
      "metadata": {
        "id": "LjW9bCkouv8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model comparison"
      ],
      "metadata": {
        "id": "8EAWAy_LwHlV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compare you model with others\n",
        "# you don't need to re-run all other experiments, instead, you can directly refer the metrics/numbers in the paper"
      ],
      "metadata": {
        "id": "uOdhGrbwwG71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion\n",
        "\n",
        "Next steps:\n",
        "- Include additional patients (currently, only 2,000 patients are included in this prelim analysis)\n",
        "- Train models for the 5 min, 10 min, and 15 min data\n",
        "- Compute AUPRC and optimal sensitivity / specificity for comparison with original paper\n",
        "- Perform ablations:\n",
        "  - Test the perfomance of ECG, blood pressure, and EEG data separately\n",
        "  - test other optimizers\n",
        "  - test different learning rates\n",
        "  - test different batch sizes"
      ],
      "metadata": {
        "id": "qH75TNU71eRH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "1.   Sun, J, [paper title], [journal title], [year], [volume]:[issue], doi: [doi link to paper]\n",
        "\n"
      ],
      "metadata": {
        "id": "SHMI2chl9omn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feel free to add new sections"
      ],
      "metadata": {
        "id": "xmVuzQ724HbO"
      }
    }
  ]
}