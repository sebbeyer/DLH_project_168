{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "V5BXdNQUyU6i",
        "eZzwBEChzQwE",
        "05gRKFjezXRW",
        "iar6E_LBzlkJ",
        "u2Dc4tl4zu12",
        "mxpMjlyPz3KZ"
      ],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Before you use this template\n",
        "\n",
        "This template is just a recommended template for project Report. It only considers the general type of research in our paper pool. Feel free to edit it to better fit your project. You will iteratively update the same notebook submission for your draft and the final submission. Please check the project rubriks to get a sense of what is expected in the template.\n",
        "\n",
        "---\n",
        "\n",
        "# FAQ and Attentions\n",
        "* Copy and move this template to your Google Drive. Name your notebook by your team ID (upper-left corner). Don't eidt this original file.\n",
        "* This template covers most questions we want to ask about your reproduction experiment. You don't need to exactly follow the template, however, you should address the questions. Please feel free to customize your report accordingly.\n",
        "* any report must have run-able codes and necessary annotations (in text and code comments).\n",
        "* The notebook is like a demo and only uses small-size data (a subset of original data or processed data), the entire runtime of the notebook including data reading, data process, model training, printing, figure plotting, etc,\n",
        "must be within 8 min, otherwise, you may get penalty on the grade.\n",
        "  * If the raw dataset is too large to be loaded  you can select a subset of data and pre-process the data, then, upload the subset or processed data to Google Drive and load them in this notebook.\n",
        "  * If the whole training is too long to run, you can only set the number of training epoch to a small number, e.g., 3, just show that the training is runable.\n",
        "  * For results model validation, you can train the model outside this notebook in advance, then, load pretrained model and use it for validation (display the figures, print the metrics).\n",
        "* The post-process is important! For post-process of the results,please use plots/figures. The code to summarize results and plot figures may be tedious, however, it won't be waste of time since these figures can be used for presentation. While plotting in code, the figures should have titles or captions if necessary (e.g., title your figure with \"Figure 1. xxxx\")\n",
        "* There is not page limit to your notebook report, you can also use separate notebooks for the report, just make sure your grader can access and run/test them.\n",
        "* If you use outside resources, please refer them (in any formats). Include the links to the resources if necessary."
      ],
      "metadata": {
        "id": "j01aH0PR4Sg-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Notebook to Google Drive\n",
        "Upload the data, pretrianed model, figures, etc to your Google Drive, then mount this notebook to Google Drive. After that, you can access the resources freely.\n",
        "\n",
        "Instruction: https://colab.research.google.com/notebooks/io.ipynb\n",
        "\n",
        "Example: https://colab.research.google.com/drive/1srw_HFWQ2SMgmWIawucXfusGzrj1_U0q\n",
        "\n",
        "Video: https://www.youtube.com/watch?v=zc8g8lGcwQU"
      ],
      "metadata": {
        "id": "dlv6knX04FiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "sfk8Zrul_E8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ],
      "metadata": {
        "id": "vU6MOdHI12bL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paper studied: Yo, Jang, Kwon, Lee, Jung, Byun, Jeong: ‘Predicting intraoperative hypotension using deep learning with waveforms of arterial blood pressure, electroencephalogram, and electrocardiogram: Retrospective study’ Plos One. https://doi.org/10.1371/journal.pone.0272055 [1]\n",
        "\n",
        "\n",
        "*Background*\n",
        "\n",
        "Surgery is often associated with fluctuations in blood pressure.(Salmasi V, Maheshwari K, Yang D, Mascha EJ, Singh A, Sessler DI, et al. Relationship between intraoperative hypotension, defined by either reduction from baseline or absolute thresholds, and acute kidney and myocardial injury after noncardiac surgery: a retrospective cohort analysis. Anesthesiology. 2017;126(1):47–65. pmid:27792044 [2]) Low blood pressure (‘intraoperative hypotension’) might occur due to medications administered (such as sedation) or blood loss. Hypotension is potentially dangerous as it might lead to reduced blood flow to vital organs such as the heart or the brain. Therefore, careful monitoring of the intraoperative blood pressure is performed in order to treat hypotensive events (usually defined as a mean arterial blood pressure [MAP] <65 mmHg) if they occur (commonly with fluids or medications). Continuously measured parameters such as the MAP, the patient’s ECG, or EEG might allow for earlier prediction of subsequent hypotensive events. This could, in turn, enable a more timely intervention and potentially even prevention of hypotensive events.\n",
        "\n",
        "\n",
        "*Paper explanation*\n",
        "\n",
        "Specific approach: This paper uses a public data repository of vital signs taken during surgery in 10 operating rooms at Seoul National University Hospital between 01/06/2005 and 03/01/2024. The final analysis included 14,140 patients undergoing non-cardiac surgery. Arterial blood pressure (ABP), Electrocardiogram (ECG), and Electroencephalogram (EEG) waveforms obtained during surgery were used to predict hypotensive events. Specifically, 1-min intervals of the waveforms were sampled 3, 5, 10, and 15 min before a hypotensive event (defined as a MAP<65 mmHg ≥1 min) and compared to waveforms prior to ‘non-events’ (samples in the middle of a 30 min window of a MAP ≥75 mmHg). Unreliable cases were removed using the J signal quality index (Li Q., Mark R.G. & Clifford G.D. Artificial arterial blood pressure artifact models and an evaluation of a robust blood pressure and heart rate estimator. BioMed Eng OnLine. 2009; 8(13). pmid:19586547).\n",
        "Following data preprocessing, the authors trained a ResNet CNN for each waveform. The outputs are subsequently concatenated and passed through a classifier to predict hypotensive events.\n"
      ],
      "metadata": {
        "id": "MQ0sNuMePBXx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scope of Reproducibility:"
      ],
      "metadata": {
        "id": "koOPr9GK2CXj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Hypothesis:*\n",
        "\n",
        "Is it possible to predict intraoperative hypotensive events using a deep-learning based analysis of MAP, ECG, and EEG waveforms?\n",
        "\n",
        "*Experimental setup:*\n",
        "\n",
        "Individual analysis of the predictive performance of the ABP, ECG, and EEG data compared to the model concatenating the results in order to understand the benefit vs the cost associated with the additional computation. Additional ablations will include studying the effect of the optimizer (Adam was used for the final model) and the learning rate (0.0001 in the paper).\n"
      ],
      "metadata": {
        "id": "uygL9tTPSVHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methodology\n"
      ],
      "metadata": {
        "id": "xWAHJ_1CdtaA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Link to github repo: https://github.com/sebbeyer/DLH_project_168.git\n",
        "\n",
        "Link to video presentation: https://mediaspace.illinois.edu/media/1_iclu5qwi"
      ],
      "metadata": {
        "id": "fbXThQzRLeAr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Install and load packages**"
      ],
      "metadata": {
        "id": "NbQQoc8TLhNv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vitaldb"
      ],
      "metadata": {
        "id": "Pt8hsfY3SJud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-plot"
      ],
      "metadata": {
        "id": "TLuMo9kmeINT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import  packages you need\n",
        "import os\n",
        "import sys\n",
        "import glob\n",
        "import math\n",
        "import copy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "import vitaldb\n",
        "import scipy.signal\n",
        "import scipy.io.wavfile\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
        "from sklearn.metrics import auc\n",
        "import scikitplot as skplt\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "yu61Jp1xrnKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set seed\n",
        "seed = 99\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(seed)"
      ],
      "metadata": {
        "id": "FDN3ehurQ4nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  **Load Data**\n",
        "\n",
        "*Source of the data:*\n",
        "\n",
        "The dataset used is open access: https://osf.io/dtc45/. It can be obtained after signing the Data Use Agreement (https://vitaldb.net/docs/?documentId=1OyhiDYbN-VJ6TOme-Fkj4wbqJkVT3UazELcbCXcHmiY)\n",
        "\n",
        "To run the notebook:\n",
        "1) Download the .vital files after signing the Data Use Agreement (each .vital file corresponds to the tracings of one patient during surgery)\n",
        "2) update 'raw_data_dir'\n",
        "\n",
        "**The currently available dataset includes intraoperative recordings from 6,388 patients (https://vitaldb.net/dataset/). Interstingly, the original paper included 39,000 cases in the Vital DB. I am not sure whether the databse has been updated or whether the authors had access to additional cases. For this analysis, the currently available 6,388 patients will be included**"
      ],
      "metadata": {
        "id": "2NbPHUTMbkD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # show available tracks (all vital sign tracings obtained during surgery)\n",
        "# tracks = vitaldb.vital_trks('/content/drive/MyDrive/VitalDB/vital_files_1_250/0001.vital')\n",
        "# tracks"
      ],
      "metadata": {
        "id": "DFHNjHawcssd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_raw_data(raw_data_dir):\n",
        "\n",
        "  # load data to lists with events and non-events (controls)\n",
        "  # generate separate lists for 3, 5, 10, and 15 min preceding events\n",
        "\n",
        "  ecg_3min = []\n",
        "  art_3min = []\n",
        "  eeg_3min = []\n",
        "  y_3min = []\n",
        "\n",
        "  ecg_5min = []\n",
        "  art_5min = []\n",
        "  eeg_5min = []\n",
        "  y_5min = []\n",
        "\n",
        "  ecg_10min = []\n",
        "  art_10min = []\n",
        "  eeg_10min = []\n",
        "  y_10min = []\n",
        "\n",
        "  ecg_15min = []\n",
        "  art_15min = []\n",
        "  eeg_15min = []\n",
        "  y_15min = []\n",
        "\n",
        "  # tracks to load\n",
        "  tracks = ['SNUADC/ECG_II', 'SNUADC/ART', 'BIS/EEG1_WAV'] # the paper does not specify which ECG lead they used for the analysis -> Here, we are using lead II\n",
        "  # the paper also does not specify which EEG waveform is being used -> Here, we are using waveform 1\n",
        "\n",
        "  # iterate over all files\n",
        "  for file in os.listdir(raw_data_dir):\n",
        "\n",
        "    #  extract ecg, blood pressure (art) and eeg data as numpy array\n",
        "    vital_object = vitaldb.VitalFile(file, tracks)\n",
        "    ecg_np = vital_object.to_numpy('SNUADC/ECG_II', 1/500)\n",
        "    art_np = vital_object.to_numpy('SNUADC/ART', 1/500)\n",
        "    eeg_np = vital_object.to_numpy('BIS/EEG1_WAV', 1/128)\n",
        "\n",
        "    # calculate mean arterial blood pressure (MAP) for 1 minute intervals\n",
        "    map = []\n",
        "\n",
        "    for i in range(0, art_np.shape[0], 30000): # sampling of 500Hz -> 30,000 datapoints / min\n",
        "      map.append(np.mean(art_np[i:i+30000]))\n",
        "\n",
        "    map = np.asarray(map)\n",
        "\n",
        "    # Quality check - exclude implausible values\n",
        "    map = np.where(map<20, np.nan, map)\n",
        "    map = np.where(map>200, np.nan, map)\n",
        "\n",
        "    # find index of events\n",
        "    # hypotensive events: MAP<65 mmHg\n",
        "\n",
        "    # index of hypotensive events mandating >20 min between each\n",
        "    # hypotensive event (the paper does not specify whether this is\n",
        "    # 20 min after the hypotension has completely resolved or 20 min\n",
        "    # after the onset of hypotension)\n",
        "    # -> here, I will require >20 min following the resolution of\n",
        "    # hypotension\n",
        "\n",
        "    # even more importantly, the authors did not specify how the following\n",
        "    # scenario should be handled: a second hypotensive event w/in 20 min\n",
        "    # of a prior hypotensive event with a third hypotensive event w/in 20 min\n",
        "    # of the second hypotensive event but >20 min after the first hypotensive\n",
        "    # event: should the third hypotensive event be considered an event and\n",
        "    # included in the analysis???\n",
        "    # -> here, I excluded even the third event as there are <20 min between\n",
        "    # consecutive hypotensive events\n",
        "\n",
        "    # the authors also didn't specify how they dealt with MAP>75 mmHg\n",
        "    # segemnts lasting >30 (i.e. when during those longer interavls they\n",
        "    # sampled controls)\n",
        "    # -> here, I will use the initial 30 min of such intervals\n",
        "\n",
        "    # 'events' array as indicator array:\n",
        "      # '0' : MAP 65 - 75 mmHg\n",
        "      # '1': MAP <65 mmHg with >20 min since the last hypotensive event\n",
        "      # '-1': MAP <65 mmHg with <= 20 min since last hypotensive event\n",
        "      # '2': MAP > 75 mmHg for 30 min (initial 30 min if MAP>75 for >30 min)\n",
        "\n",
        "    events = (map<65)*1 # MAP<65\n",
        "    events = np.where(np.isnan(map), np.nan, events) # keep nan as nan intervals\n",
        "\n",
        "    prec_20_min = np.asarray([np.nansum(events[max(i[0]-20, 0):i[0]]) for i in enumerate(events)]) # check for hypotensive events during 20 min interval preceding that timestamp\n",
        "    prec_20_min[prec_20_min >0] = 2\n",
        "\n",
        "    events = events - prec_20_min\n",
        "    events[events < -1] = 0\n",
        "\n",
        "    map_75 = (map>75)*2 # MAP>75\n",
        "    map_75 = np.where(np.isnan(map), np.nan, map_75) # keep nan as nan intervals\n",
        "\n",
        "    prec_30_min = np.asarray([np.nansum(map_75[max(i[0]-30, 0):i[0]]) for i in enumerate(map_75)]) # check for MAP>75 lasting >= 30 min\n",
        "    prec_30_min = (prec_30_min == 60).astype(int)\n",
        "\n",
        "    prec_30_min_2 = np.asarray([np.nansum(prec_30_min[max(i[0]-30, 0):i[0]]) for i in enumerate(prec_30_min)]) # identify initial 30 min w/ MAP >75 if longer time interval >75 mmHg\n",
        "    prec_30_min_2[prec_30_min_2 >0] = 2\n",
        "\n",
        "    map_75 = (map_75 * prec_30_min) - prec_30_min_2\n",
        "    map_75[map_75 == -2] = 0\n",
        "\n",
        "    events = events + map_75\n",
        "\n",
        "    # append lists\n",
        "\n",
        "    for i, j in enumerate(events):\n",
        "      if j == 1:\n",
        "        if (i >2 and np.isnan(ecg_np[(i-3)*30000:(i-2)*30000]).any() == False and\n",
        "        np.isnan(art_np[(i-3)*30000:(i-2)*30000]).any() == False and\n",
        "        np.isnan(eeg_np[(i-3)*7680:(i-2)*7680]).any() == False and\n",
        "        np.isnan(map[i-3]) == False):\n",
        "\n",
        "          ecg_3min.append(ecg_np[(i-3)*30000:(i-2)*30000])\n",
        "          art_3min.append(art_np[(i-3)*30000:(i-2)*30000])\n",
        "          eeg_3min.append(eeg_np[(i-3)*7680:(i-2)*7680])\n",
        "          y_3min.append(1)\n",
        "\n",
        "        if (i >4 and np.isnan(ecg_np[(i-5)*30000:(i-4)*30000]).any() == False and\n",
        "        np.isnan(art_np[(i-5)*30000:(i-4)*30000]).any() == False and\n",
        "        np.isnan(eeg_np[(i-5)*7680:(i-4)*7680]).any() == False and\n",
        "        np.isnan(map[i-5]) == False):\n",
        "\n",
        "          ecg_5min.append(ecg_np[(i-5)*30000:(i-4)*30000])\n",
        "          art_5min.append(art_np[(i-5)*30000:(i-4)*30000])\n",
        "          eeg_5min.append(eeg_np[(i-5)*7680:(i-4)*7680])\n",
        "          y_5min.append(1)\n",
        "\n",
        "        if (i >9 and np.isnan(ecg_np[(i-10)*30000:(i-9)*30000]).any() == False and\n",
        "        np.isnan(art_np[(i-10)*30000:(i-9)*30000]).any() == False and\n",
        "        np.isnan(eeg_np[(i-10)*7680:(i-9)*7680]).any() == False and\n",
        "        np.isnan(map[i-10]) == False):\n",
        "\n",
        "          ecg_10min.append(ecg_np[(i-10)*30000:(i-9)*30000])\n",
        "          art_10min.append(art_np[(i-10)*30000:(i-9)*30000])\n",
        "          eeg_10min.append(eeg_np[(i-10)*7680:(i-9)*7680])\n",
        "          y_10min.append(1)\n",
        "\n",
        "        if (i >14 and np.isnan(ecg_np[(i-15)*30000:(i-14)*30000]).any() == False and\n",
        "        np.isnan(art_np[(i-15)*30000:(i-14)*30000]).any() == False and\n",
        "        np.isnan(eeg_np[(i-15)*7680:(i-14)*7680]).any() == False and\n",
        "        np.isnan(map[i-15]) == False):\n",
        "\n",
        "          ecg_15min.append(ecg_np[(i-15)*30000:(i-14)*30000])\n",
        "          art_15min.append(art_np[(i-15)*30000:(i-14)*30000])\n",
        "          eeg_15min.append(eeg_np[(i-15)*7680:(i-14)*7680])\n",
        "          y_15min.append(1)\n",
        "\n",
        "      if (j == 2 and np.isnan(ecg_np[(i-15)*30000:(i-14)*30000]).any() == False and\n",
        "      np.isnan(art_np[(i-15)*30000:(i-14)*30000]).any() == False and\n",
        "      np.isnan(eeg_np[(i-15)*7680:(i-14)*7680]).any() == False and\n",
        "      np.isnan(map[i-15]) == False):\n",
        "\n",
        "        ecg_3min.append(ecg_np[(i-15)*30000:(i-14)*30000])\n",
        "        ecg_5min.append(ecg_np[(i-15)*30000:(i-14)*30000])\n",
        "        ecg_10min.append(ecg_np[(i-15)*30000:(i-14)*30000])\n",
        "        ecg_15min.append(ecg_np[(i-15)*30000:(i-14)*30000])\n",
        "        art_3min.append(art_np[(i-15)*30000:(i-14)*30000])\n",
        "        art_5min.append(art_np[(i-15)*30000:(i-14)*30000])\n",
        "        art_10min.append(art_np[(i-15)*30000:(i-14)*30000])\n",
        "        art_15min.append(art_np[(i-15)*30000:(i-14)*30000])\n",
        "        eeg_3min.append(eeg_np[(i-15)*7680:(i-14)*7680])\n",
        "        eeg_5min.append(eeg_np[(i-15)*7680:(i-14)*7680])\n",
        "        eeg_10min.append(eeg_np[(i-15)*7680:(i-14)*7680])\n",
        "        eeg_15min.append(eeg_np[(i-15)*7680:(i-14)*7680])\n",
        "        y_3min.append(0)\n",
        "        y_5min.append(0)\n",
        "        y_10min.append(0)\n",
        "        y_15min.append(0)\n",
        "\n",
        "  return (ecg_3min, art_3min, eeg_3min, y_3min, ecg_5min, art_5min, eeg_5min, y_5min,\n",
        "         ecg_10min, art_10min, eeg_10min, y_10min, ecg_15min, art_15min, eeg_15min, y_15min)\n"
      ],
      "metadata": {
        "id": "U7aDfa9_9GDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data_dir = ['/content/drive/MyDrive/VitalDB/vital_files_1_6388']\n",
        "\n",
        "ecg_3min = []\n",
        "art_3min = []\n",
        "eeg_3min = []\n",
        "y_3min = []\n",
        "\n",
        "ecg_5min = []\n",
        "art_5min = []\n",
        "eeg_5min = []\n",
        "y_5min = []\n",
        "\n",
        "ecg_10min = []\n",
        "art_10min = []\n",
        "eeg_10min = []\n",
        "y_10min = []\n",
        "\n",
        "ecg_15min = []\n",
        "art_15min = []\n",
        "eeg_15min = []\n",
        "y_15min = []\n",
        "\n",
        "for i in raw_data_dir:\n",
        "\n",
        "  os.chdir(i)\n",
        "\n",
        "  j = load_raw_data(i)\n",
        "  ecg_3min.extend(j[0])\n",
        "  art_3min.extend(j[1])\n",
        "  eeg_3min.extend(j[2])\n",
        "  y_3min.extend(j[3])\n",
        "\n",
        "  ecg_5min.extend(j[4])\n",
        "  art_5min.extend(j[5])\n",
        "  eeg_5min.extend(j[6])\n",
        "  y_5min.extend(j[7])\n",
        "\n",
        "  ecg_10min.extend(j[8])\n",
        "  art_10min.extend(j[9])\n",
        "  eeg_10min.extend(j[10])\n",
        "  y_10min.extend(j[11])\n",
        "\n",
        "  ecg_15min.extend(j[12])\n",
        "  art_15min.extend(j[13])\n",
        "  eeg_15min.extend(j[14])\n",
        "  y_15min.extend(j[15])"
      ],
      "metadata": {
        "id": "TY_7F2sbAWl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert to np array and save copy to google drive\n",
        "\n",
        "ecg_3min = np.asarray(ecg_3min)\n",
        "art_3min = np.asarray(art_3min)\n",
        "eeg_3min = np.asarray(eeg_3min)\n",
        "y_3min = np.asarray(y_3min)\n",
        "\n",
        "ecg_5min = np.asarray(ecg_5min)\n",
        "art_5min = np.asarray(art_5min)\n",
        "eeg_5min = np.asarray(eeg_5min)\n",
        "y_5min = np.asarray(y_5min)\n",
        "\n",
        "ecg_10min = np.asarray(ecg_10min)\n",
        "art_10min = np.asarray(art_10min)\n",
        "eeg_10min = np.asarray(eeg_10min)\n",
        "y_10min = np.asarray(y_10min)\n",
        "\n",
        "ecg_15min = np.asarray(ecg_15min)\n",
        "art_15min = np.asarray(art_15min)\n",
        "eeg_15min = np.asarray(eeg_15min)\n",
        "y_15min = np.asarray(y_15min)\n",
        "\n",
        "os.chdir('/content/drive/MyDrive/VitalDB')\n",
        "\n",
        "np.save('/content/drive/MyDrive/VitalDB/ecg_3min_6388.npy', ecg_3min)\n",
        "np.save('/content/drive/MyDrive/VitalDB/art_3min_6388.npy', art_3min)\n",
        "np.save('/content/drive/MyDrive/VitalDB/eeg_3min_6388.npy', eeg_3min)\n",
        "np.save('/content/drive/MyDrive/VitalDB/y_3min_6388.npy', y_3min)\n",
        "\n",
        "np.save('/content/drive/MyDrive/VitalDB/ecg_5min_6388.npy', ecg_5min)\n",
        "np.save('/content/drive/MyDrive/VitalDB/art_5min_6388.npy', art_5min)\n",
        "np.save('/content/drive/MyDrive/VitalDB/eeg_5min_6388.npy', eeg_5min)\n",
        "np.save('/content/drive/MyDrive/VitalDB/y_5min_6388.npy', y_5min)\n",
        "\n",
        "np.save('/content/drive/MyDrive/VitalDB/ecg_10min_6388.npy', ecg_10min)\n",
        "np.save('/content/drive/MyDrive/VitalDB/art_10min_6388.npy', art_10min)\n",
        "np.save('/content/drive/MyDrive/VitalDB/eeg_10min_6388.npy', eeg_10min)\n",
        "np.save('/content/drive/MyDrive/VitalDB/y_10min_6388.npy', y_10min)\n",
        "\n",
        "np.save('/content/drive/MyDrive/VitalDB/ecg_15min_6388.npy', ecg_15min)\n",
        "np.save('/content/drive/MyDrive/VitalDB/art_15min_6388.npy', art_15min)\n",
        "np.save('/content/drive/MyDrive/VitalDB/eeg_15min_6388.npy', eeg_15min)\n",
        "np.save('/content/drive/MyDrive/VitalDB/y_15min_6388.npy', y_15min)"
      ],
      "metadata": {
        "id": "GnNpuazTDA-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load np arrays\n",
        "ecg_3min = np.load('/content/drive/MyDrive/VitalDB/ecg_3min_6388.npy')\n",
        "art_3min = np.load('/content/drive/MyDrive/VitalDB/art_3min_6388.npy')\n",
        "eeg_3min = np.load('/content/drive/MyDrive/VitalDB/eeg_3min_6388.npy')\n",
        "y_3min = np.load('/content/drive/MyDrive/VitalDB/y_3min_6388.npy')\n",
        "\n",
        "ecg_5min = np.load('/content/drive/MyDrive/VitalDB/ecg_5min_6388.npy')\n",
        "art_5min = np.load('/content/drive/MyDrive/VitalDB/art_5min_6388.npy')\n",
        "eeg_5min = np.load('/content/drive/MyDrive/VitalDB/eeg_5min_6388.npy')\n",
        "y_5min = np.load('/content/drive/MyDrive/VitalDB/y_5min_6388.npy')\n",
        "\n",
        "ecg_10min = np.load('/content/drive/MyDrive/VitalDB/ecg_10min_6388.npy')\n",
        "art_10min = np.load('/content/drive/MyDrive/VitalDB/art_10min_6388.npy')\n",
        "eeg_10min = np.load('/content/drive/MyDrive/VitalDB/eeg_10min_6388.npy')\n",
        "y_10min = np.load('/content/drive/MyDrive/VitalDB/y_10min_6388.npy')\n",
        "\n",
        "ecg_15min = np.load('/content/drive/MyDrive/VitalDB/ecg_15min_6388.npy')\n",
        "art_15min = np.load('/content/drive/MyDrive/VitalDB/art_15min_6388.npy')\n",
        "eeg_15min = np.load('/content/drive/MyDrive/VitalDB/eeg_15min_6388.npy')\n",
        "y_15min = np.load('/content/drive/MyDrive/VitalDB/y_15min_6388.npy')"
      ],
      "metadata": {
        "id": "jaEzO9ED8jMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Preprocessing**"
      ],
      "metadata": {
        "id": "eR5F1Ej-L0vH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply frequency filter\n",
        "# While the frequencies are provided in the paper, additional technical\n",
        "# details such as the type of filter or the filter settings are not mentioned\n",
        "# -> here, I am using a 4-th order Butterworth filter\n",
        "\n",
        "def bandpass(data, edges, sampling_rate, poles: int = 4):\n",
        "    sos = scipy.signal.butter(poles, edges, 'bandpass', fs=sampling_rate, output='sos')\n",
        "    filtered_data = scipy.signal.sosfiltfilt(sos, data, axis=1)\n",
        "    return filtered_data"
      ],
      "metadata": {
        "id": "umpkR4Ipp0OY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing ECGs using Z scores\n",
        "# It is not clear from the paper whether Z score are calculated for each sample\n",
        "# or for the entire dataset\n",
        "\n",
        "def normalize(data):\n",
        "\n",
        "  mean_data = np.mean(data)\n",
        "  sd_data = np.std(data)\n",
        "  normalized_data = (data - mean_data) / sd_data\n",
        "  return normalized_data"
      ],
      "metadata": {
        "id": "_QAuFWNQsTjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# process raw data\n",
        "sampling_rate_ecg = 500\n",
        "fmin_ecg = 1\n",
        "fmax_ecg = 40\n",
        "\n",
        "sampling_rate_eeg = 128\n",
        "fmin_eeg = 0.5\n",
        "fmax_eeg = 50\n",
        "\n",
        "def process_data(ecg_3min, ecg_5min, ecg_10min, ecg_15min, eeg_3min, eeg_5min, eeg_10min, eeg_15min,\n",
        "                 sampling_rate_ecg, fmin_ecg, fmax_ecg, sampling_rate_eeg, fmin_eeg, fmax_eeg):\n",
        "\n",
        "  # bandpass filter\n",
        "  ecg_3min_filtered = bandpass(ecg_3min, [fmin_ecg, fmax_ecg], sampling_rate_ecg)\n",
        "  ecg_5min_filtered = bandpass(ecg_5min, [fmin_ecg, fmax_ecg], sampling_rate_ecg)\n",
        "  ecg_10min_filtered = bandpass(ecg_10min, [fmin_ecg, fmax_ecg], sampling_rate_ecg)\n",
        "  ecg_15min_filtered = bandpass(ecg_15min, [fmin_ecg, fmax_ecg], sampling_rate_ecg)\n",
        "\n",
        "  eeg_3min_filtered = bandpass(eeg_3min, [fmin_eeg, fmax_eeg], sampling_rate_eeg)\n",
        "  eeg_5min_filtered = bandpass(eeg_5min, [fmin_eeg, fmax_eeg], sampling_rate_eeg)\n",
        "  eeg_10min_filtered = bandpass(eeg_10min, [fmin_eeg, fmax_eeg], sampling_rate_eeg)\n",
        "  eeg_15min_filtered = bandpass(eeg_15min, [fmin_eeg, fmax_eeg], sampling_rate_eeg)\n",
        "\n",
        "  # normalizing ECG using Z score\n",
        "  ecg_3min_normalized = normalize(ecg_3min_filtered)\n",
        "  ecg_5min_normalized = normalize(ecg_5min_filtered)\n",
        "  ecg_10min_normalized = normalize(ecg_10min_filtered)\n",
        "  ecg_15min_normalized = normalize(ecg_15min_filtered)\n",
        "\n",
        "  return ecg_3min_normalized, ecg_5min_normalized, ecg_10min_normalized, ecg_15min_normalized, \\\n",
        "        eeg_3min_filtered, eeg_5min_filtered, eeg_10min_filtered, eeg_15min_filtered\n",
        "\n",
        "ecg_3min_normalized, ecg_5min_normalized, ecg_10min_normalized, ecg_15min_normalized, \\\n",
        "eeg_3min_filtered, eeg_5min_filtered, eeg_10min_filtered, eeg_15min_filtered = \\\n",
        "process_data(ecg_3min, ecg_5min, ecg_10min, ecg_15min, eeg_3min, eeg_5min, eeg_10min, eeg_15min,\n",
        "                 sampling_rate_ecg, fmin_ecg, fmax_ecg, sampling_rate_eeg, fmin_eeg, fmax_eeg)"
      ],
      "metadata": {
        "id": "A6fJPhBvS9Gp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Statistics and Sample Tracings**"
      ],
      "metadata": {
        "id": "A9FqYwGDL9BO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Statistics\n",
        "\n",
        "# calculate statistics\n",
        "def calculate_stats(y):\n",
        "\n",
        "  n_samples_3min = len(y)\n",
        "  cases_3min = np.sum(y)\n",
        "  controls_3min = len(y) - np.sum(y)\n",
        "\n",
        "  return n_samples_3min, cases_3min, controls_3min\n",
        "\n",
        "n_samples_3min, cases_3min, controls_3min = calculate_stats(y_3min)\n",
        "\n",
        "print(f'total number of samples with at least 3 minutes of data prior to event: {n_samples_3min}')\n",
        "print(f'total number of cases with at least 3 minutes of data prior to event: {cases_3min}')\n",
        "print(f'total number of controls: {controls_3min}')"
      ],
      "metadata": {
        "id": "oOf4YGOYaf21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot sample waveforms\n",
        "\n",
        "ecg = ecg_3min_normalized[5500,:]\n",
        "art = art_3min[1000,:]\n",
        "eeg = eeg_3min_filtered[1000,:]\n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.subplot(311)\n",
        "plt.plot(ecg[0:1000], color='g')\n",
        "plt.subplot(312)\n",
        "plt.plot(art[0:1000], color='r')\n",
        "plt.subplot(313)\n",
        "plt.plot(eeg[0:1000], color='b')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Qm8oagw3UARL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Train / Val / Test Split and Dataset / Dataloader**"
      ],
      "metadata": {
        "id": "pi6TKR7VMDMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training, validation, and test samples (70:10:20 split)\n",
        "# Since each patient might contribute multiple samples, shuffling will not be used at this point\n",
        "\n",
        "def split_train_val_test(ecg, art, eeg, y, cutoff_train_val, cutoff_val_test):\n",
        "\n",
        "  split_train_val = int(cutoff_train_val * len(y))\n",
        "  split_val_test = int(cutoff_val_test * len(y))\n",
        "\n",
        "  ecg_train = ecg[:split_train_val]\n",
        "  ecg_val = ecg[split_train_val:split_val_test]\n",
        "  ecg_test = ecg[split_val_test:]\n",
        "\n",
        "  art_train = art[:split_train_val]\n",
        "  art_val = art[split_train_val:split_val_test]\n",
        "  art_test = art[split_val_test:]\n",
        "\n",
        "  eeg_train = eeg[:split_train_val]\n",
        "  eeg_val = eeg[split_train_val:split_val_test]\n",
        "  eeg_test = eeg[split_val_test:]\n",
        "\n",
        "  y_train = y[:split_train_val]\n",
        "  y_val = y[split_train_val:split_val_test]\n",
        "  y_test = y[split_val_test:]\n",
        "\n",
        "  return ecg_train, ecg_val, ecg_test, art_train, art_val, art_test, eeg_train, eeg_val, eeg_test, y_train, y_val, y_test\n",
        "\n",
        "ecg_3min_train, ecg_3min_val, ecg_3min_test, art_3min_train, art_3min_val, art_3min_test, eeg_3min_train, eeg_3min_val, eeg_3min_test, y_3min_train, y_3min_val, y_3min_test = \\\n",
        "split_train_val_test(ecg_3min, art_3min, eeg_3min, y_3min, 0.7, 0.8)\n",
        "\n",
        "ecg_5min_train, ecg_5min_val, ecg_5min_test, art_5min_train, art_5min_val, art_5min_test, eeg_5min_train, eeg_5min_val, eeg_5min_test, y_5min_train, y_5min_val, y_5min_test = \\\n",
        "split_train_val_test(ecg_5min_normalized, art_5min, eeg_5min_filtered, y_5min, 0.7, 0.8)\n",
        "\n",
        "ecg_10min_train, ecg_10min_val, ecg_10min_test, art_10min_train, art_10min_val, art_10min_test, eeg_10min_train, eeg_10min_val, eeg_10min_test, y_10min_train, y_10min_val, y_10min_test = \\\n",
        "split_train_val_test(ecg_10min_normalized, art_10min, eeg_10min_filtered, y_10min, 0.7, 0.8)\n",
        "\n",
        "ecg_15min_train, ecg_15min_val, ecg_15min_test, art_15min_train, art_15min_val, art_15min_test, eeg_15min_train, eeg_15min_val, eeg_15min_test, y_15min_train, y_15min_val, y_15min_test = \\\n",
        "split_train_val_test(ecg_15min_normalized, art_15min, eeg_15min_filtered, y_15min, 0.7, 0.8)"
      ],
      "metadata": {
        "id": "nhqrqezcbUht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Custom Dataset class\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class HypoDataset(Dataset):\n",
        "\n",
        "    def __init__(self, ecg, art, eeg, y):\n",
        "\n",
        "        super().__init__()\n",
        "        self.y = y\n",
        "        self.ecg = ecg\n",
        "        self.art = art\n",
        "        self.eeg = eeg\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "\n",
        "        return ((self.ecg[i], self.art[i], self.eeg[i]), self.y[i])"
      ],
      "metadata": {
        "id": "P_-hk9rpRDLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to load dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def load_data(dataset, batch_size=128):\n",
        "    \"\"\"\n",
        "    Return a DataLoader instance basing on a Dataset instance, with batch_size specified.\n",
        "    \"\"\"\n",
        "    def my_collate(batch):\n",
        "\n",
        "        # your code here\n",
        "        x, y = zip(*batch)\n",
        "        ecg, art, eeg = zip(*x)\n",
        "\n",
        "        Y = torch.tensor(y, dtype=torch.float)\n",
        "\n",
        "        ECG = torch.tensor(np.asarray(ecg), dtype=torch.float).transpose(1,2)\n",
        "        ART = torch.tensor(np.asarray(art), dtype=torch.float).transpose(1,2)\n",
        "        EEG = torch.tensor(np.asarray(eeg), dtype=torch.float).transpose(1,2)\n",
        "\n",
        "        return (ECG, ART, EEG), Y\n",
        "\n",
        "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=my_collate)"
      ],
      "metadata": {
        "id": "QfxjIvPV__Ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader_3min = load_data(HypoDataset(ecg_3min_train, art_3min_train, eeg_3min_train, y_3min_train), batch_size=128)\n",
        "val_loader_3min = load_data(HypoDataset(ecg_3min_val, art_3min_val, eeg_3min_val, y_3min_val), batch_size=128)\n",
        "test_loader_3min = load_data(HypoDataset(ecg_3min_test, art_3min_test, eeg_3min_test, y_3min_test), batch_size=128)\n",
        "\n",
        "train_loader_5min = load_data(HypoDataset(ecg_5min_train, art_5min_train, eeg_5min_train, y_5min_train), batch_size=128)\n",
        "val_loader_5min = load_data(HypoDataset(ecg_5min_val, art_5min_val, eeg_5min_val, y_5min_val), batch_size=128)\n",
        "test_loader_5min = load_data(HypoDataset(ecg_5min_test, art_5min_test, eeg_5min_test, y_5min_test), batch_size=128)\n",
        "\n",
        "train_loader_10min = load_data(HypoDataset(ecg_10min_train, art_10min_train, eeg_10min_train, y_10min_train), batch_size=128)\n",
        "val_loader_10min = load_data(HypoDataset(ecg_10min_val, art_10min_val, eeg_10min_val, y_10min_val), batch_size=128)\n",
        "test_loader_10min = load_data(HypoDataset(ecg_10min_test, art_10min_test, eeg_10min_test, y_10min_test), batch_size=128)\n",
        "\n",
        "train_loader_15min = load_data(HypoDataset(ecg_15min_train, art_15min_train, eeg_15min_train, y_15min_train), batch_size=128)\n",
        "val_loader_15min = load_data(HypoDataset(ecg_15min_val, art_15min_val, eeg_15min_val, y_15min_val), batch_size=128)\n",
        "test_loader_15min = load_data(HypoDataset(ecg_15min_test, art_15min_test, eeg_15min_test, y_15min_test), batch_size=128)"
      ],
      "metadata": {
        "id": "WW_cwWQjDiBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Architecture**"
      ],
      "metadata": {
        "id": "p7L2OMzrqhxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the model architecture and details are poorly described and\n",
        "# discrepant. quite frankly, this makes me question the choice of\n",
        "# this paper as an option for a final project in this class:\n",
        "\n",
        "# 1)\n",
        "# the text mentions an additional encoder block (conv + dropout)\n",
        "# (before the data are passed through the residual blocks), which\n",
        "# is not shown in Fig 2 or Suppl. Table 1. Even more concerning, in the\n",
        "# text it says that the encoder blocks consist of a conv layer and a\n",
        "# max pooling layer whose technical specifications aren't mentioned\n",
        "# at all\n",
        "\n",
        "# 2)\n",
        "# at least some of the conv layers have to use padding since residual\n",
        "# connections are being used. However, padding is not mentioned at all\n",
        "\n",
        "# 3)\n",
        "# Supple Table 1 (detailing the hyperparameter settings) is not\n",
        "# consistent with the description of the model in the text of Fig 2:\n",
        "# the output size in Suppl Table 1 implies pooling layers are being\n",
        "# used between every other residual layer, but this is inconsistent\n",
        "# with the text or figure\n",
        "\n",
        "# 4)\n",
        "# According to Suppl. Table 1, channels increases w/ subsequent residual blocks,\n",
        "# but according to Fig 2 each layer has to conv layers - ???which layer\n",
        "# increases the channels???\n",
        "\n",
        "# 5)\n",
        "# According to Suppl. Table 1, kernel sizes change with subsequent\n",
        "# residual blocks - this is contradictory to what is mentioned in the text\n",
        "\n",
        "# 6)\n",
        "# What kind of activation function do the linear layers have? Relu???\n",
        "\n",
        "# 7)\n",
        "# Some residual blocks increase the number of channels -> a residual\n",
        "# connection is not possible here (unless the number of channels of the\n",
        "# input is also being adjusted for the skip connection, which is\n",
        "# also not mentioned at all in the text)\n",
        "\n",
        "# 8)\n",
        "# The output size numbers provided in Suppl Table 1 don't add up:\n",
        "# Max pooling w/ (2,2) applied to 1875 results in 937, not 938\n",
        "# Max pooling w/ (2,2) applied to 937 results in 468 (and even\n",
        "# max pooling applied to 938 does not result in 496)\n",
        "\n",
        "# 9)\n",
        "# it is unclear how weights were initialized\n",
        "\n",
        "# 10)\n",
        "# The authors do not mention the software / package they used for the analysis!\n",
        "\n",
        "class my_model(nn.Module):\n",
        "  # use this class to define your model\n",
        "  def __init__(self):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder_ecg = nn.Sequential(\n",
        "                        nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer1_ecg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(1),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(1,2,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_ecg_layer1 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer2_ecg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer3_ecg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_ecg_layer3 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer4_ecg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer5_ecg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_ecg_layer5 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer6_ecg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,4,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(4,4,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer7_ecg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(4,4,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(4,4,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_ecg_layer7 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer8_ecg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(4,4,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(4,4,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer9_ecg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(4,4,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(4,4,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_ecg_layer9 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer10_ecg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(4,6,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(6,6,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer11_ecg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(6,6,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(6,6,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_ecg_layer11 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer12_ecg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(6,6,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.ReLU(),\n",
        "                        )\n",
        "\n",
        "    self.linear_ecg = nn.Linear(468*6, 32)\n",
        "\n",
        "    self.encoder_art = nn.Sequential(\n",
        "                        nn.Conv1d(1,1,3,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer1_art = nn.Sequential(\n",
        "                        nn.BatchNorm1d(1),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(1,2,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_art_layer1 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer2_art = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer3_art = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_art_layer3 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer4_art = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer5_art = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_art_layer5 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer6_art = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,4,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(4,4,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer7_art = nn.Sequential(\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(4,4,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(4,4,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_art_layer7 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer8_art = nn.Sequential(\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(4,4,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(4,4,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer9_art = nn.Sequential(\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(4,4,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(4,4,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_art_layer9 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer10_art = nn.Sequential(\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(4,6,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(6,6,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer11_art = nn.Sequential(\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(6,6,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(6,6,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_art_layer11 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer12_art = nn.Sequential(\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(6,6,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.ReLU(),\n",
        "                        )\n",
        "\n",
        "    self.linear_art = nn.Linear(468*6, 32)\n",
        "\n",
        "    self.encoder_eeg = nn.Sequential(\n",
        "                        nn.Conv1d(1,1,3,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer1_eeg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(1),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(1,2,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_eeg_layer1 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer2_eeg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer3_eeg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_eeg_layer3 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer4_eeg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer5_eeg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_eeg_layer5 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer6_eeg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,4,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(4,4,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer7_eeg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(4,4,3,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(4,4,3,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_eeg_layer7 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer8_eeg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(4,4,3,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(4,4,3,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer9_eeg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(4,4,3,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(4,4,3,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_eeg_layer9 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer10_eeg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(4,6,3,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(6,6,3,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer11_eeg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(6,6,3,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(6,6,3,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_eeg_layer11 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer12_eeg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(6,6,3,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.ReLU(),\n",
        "                        )\n",
        "\n",
        "    self.linear_eeg = nn.Linear(120*6, 32)\n",
        "\n",
        "    self.linear_combined1 = nn.Linear(96, 16)\n",
        "    self.linear_combined2 = nn.Linear(16, 1)\n",
        "\n",
        "    self.apply(self._init_weights)\n",
        "\n",
        "  def _init_weights(self, module):\n",
        "\n",
        "      if isinstance(module, nn.Conv1d):\n",
        "        nn.init.kaiming_normal_(module.weight, nonlinearity='relu')\n",
        "\n",
        "        if module.bias is not None:\n",
        "          nn.init.zeros_(module.bias)\n",
        "\n",
        "      elif isinstance(module, nn.BatchNorm1d):\n",
        "        nn.init.constant_(module.weight, 1)\n",
        "        nn.init.constant_(module.bias, 0)\n",
        "\n",
        "      elif isinstance(module, nn.Linear):\n",
        "        nn.init.xavier_normal_(module.weight)\n",
        "\n",
        "        if module.bias is not None:\n",
        "          nn.init.zeros_(module.bias)\n",
        "\n",
        "  def forward(self, ecg, art, eeg):\n",
        "\n",
        "    ecg = self.encoder_ecg(ecg)\n",
        "    tmp = self.layer1_ecg(ecg)\n",
        "    ecg = tmp + ecg\n",
        "    ecg = self.maxpool_ecg_layer1(ecg)\n",
        "    tmp = self.layer2_ecg(ecg)\n",
        "    ecg = tmp + ecg\n",
        "    tmp = self.layer3_ecg(ecg)\n",
        "    ecg = tmp + ecg\n",
        "    ecg = self.maxpool_ecg_layer3(ecg)\n",
        "    tmp = self.layer4_ecg(ecg)\n",
        "    ecg = tmp + ecg\n",
        "    tmp = self.layer5_ecg(ecg)\n",
        "    ecg = tmp + ecg\n",
        "    ecg = self.maxpool_ecg_layer5(ecg)\n",
        "    ecg = self.layer6_ecg(ecg)\n",
        "    tmp = self.layer7_ecg(ecg)\n",
        "    ecg = tmp + ecg\n",
        "    ecg = self.maxpool_ecg_layer7(ecg)\n",
        "    tmp = self.layer8_ecg(ecg)\n",
        "    ecg = tmp + ecg\n",
        "    tmp = self.layer9_ecg(ecg)\n",
        "    ecg = tmp + ecg\n",
        "    ecg = self.maxpool_ecg_layer9(ecg)\n",
        "    ecg = self.layer10_ecg(ecg)\n",
        "    tmp = self.layer11_ecg(ecg)\n",
        "    ecg = tmp + ecg\n",
        "    ecg = self.maxpool_ecg_layer11(ecg)\n",
        "    tmp = self.layer12_ecg(ecg)\n",
        "    ecg = tmp + ecg\n",
        "    ecg = torch.flatten(ecg, 1)\n",
        "    ecg = F.relu(self.linear_ecg(ecg))\n",
        "\n",
        "    art = self.encoder_art(art)\n",
        "    tmp = self.layer1_art(art)\n",
        "    art = tmp + art\n",
        "    art = self.maxpool_art_layer1(art)\n",
        "    tmp = self.layer2_art(art)\n",
        "    art = tmp + art\n",
        "    tmp = self.layer3_art(art)\n",
        "    art = tmp + art\n",
        "    art = self.maxpool_art_layer3(art)\n",
        "    tmp = self.layer4_art(art)\n",
        "    art = tmp + art\n",
        "    tmp = self.layer5_art(art)\n",
        "    art = tmp + art\n",
        "    art = self.maxpool_art_layer5(art)\n",
        "    art = self.layer6_art(art)\n",
        "    tmp = self.layer7_art(art)\n",
        "    art = tmp + art\n",
        "    art = self.maxpool_art_layer7(art)\n",
        "    tmp = self.layer8_art(art)\n",
        "    art = tmp + art\n",
        "    tmp = self.layer9_art(art)\n",
        "    art = tmp + art\n",
        "    art = self.maxpool_art_layer9(art)\n",
        "    art = self.layer10_art(art)\n",
        "    tmp = self.layer11_art(art)\n",
        "    art = tmp + art\n",
        "    art = self.maxpool_art_layer11(art)\n",
        "    tmp = self.layer12_art(art)\n",
        "    art = tmp + art\n",
        "    art = torch.flatten(art, 1)\n",
        "    art = F.relu(self.linear_art(art))\n",
        "\n",
        "    eeg = self.encoder_eeg(eeg)\n",
        "    tmp = self.layer1_eeg(eeg)\n",
        "    eeg = tmp + eeg\n",
        "    eeg = self.maxpool_eeg_layer1(eeg)\n",
        "    tmp = self.layer2_eeg(eeg)\n",
        "    eeg = tmp + eeg\n",
        "    tmp = self.layer3_eeg(eeg)\n",
        "    eeg = tmp + eeg\n",
        "    eeg = self.maxpool_eeg_layer3(eeg)\n",
        "    tmp = self.layer4_eeg(eeg)\n",
        "    eeg = tmp + eeg\n",
        "    tmp = self.layer5_eeg(eeg)\n",
        "    eeg = tmp + eeg\n",
        "    eeg = self.maxpool_eeg_layer5(eeg)\n",
        "    eeg = self.layer6_eeg(eeg)\n",
        "    tmp = self.layer7_eeg(eeg)\n",
        "    eeg = tmp + eeg\n",
        "    eeg = self.maxpool_eeg_layer7(eeg)\n",
        "    tmp = self.layer8_eeg(eeg)\n",
        "    eeg = tmp + eeg\n",
        "    tmp = self.layer9_eeg(eeg)\n",
        "    eeg = tmp + eeg\n",
        "    eeg = self.maxpool_eeg_layer9(eeg)\n",
        "    eeg = self.layer10_eeg(eeg)\n",
        "    tmp = self.layer11_eeg(eeg)\n",
        "    eeg = tmp + eeg\n",
        "    eeg = self.maxpool_eeg_layer11(eeg)\n",
        "    tmp = self.layer12_eeg(eeg)\n",
        "    eeg = tmp + eeg\n",
        "    eeg = torch.flatten(eeg, 1)\n",
        "    eeg = F.relu(self.linear_eeg(eeg))\n",
        "\n",
        "    combined = F.relu(self.linear_combined1(torch.cat((ecg, art, eeg), -1)))\n",
        "    logits = self.linear_combined2(combined)\n",
        "\n",
        "    return logits"
      ],
      "metadata": {
        "id": "gBdVZoTvsSFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "q0UrnXzPqqim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define function to train for one epoch and return the model state,\n",
        "# epoch training loss, and epoch training accuracy\n",
        "\n",
        "def train_model_one_iter(train_dataloader, model, loss_func, optimizer):\n",
        "\n",
        "  model.train()\n",
        "  running_loss = 0\n",
        "  total_correct = 0\n",
        "  total_samples = 0\n",
        "\n",
        "  for (ecg, art, eeg), y in train_dataloader:\n",
        "\n",
        "    logits = model(ecg, art, eeg)\n",
        "    loss = loss_func(logits.view(logits.shape[0]), y)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()*y.size(0)\n",
        "\n",
        "    # calculate accuracy for current batch\n",
        "    y_hat = torch.sigmoid(model(ecg, art, eeg))\n",
        "    y_hat = y_hat.detach()\n",
        "    y_hat = (y_hat>0.5).int()\n",
        "    total_correct += torch.sum(y_hat.view(y_hat.shape[0]) == y)\n",
        "    total_samples += y.size(0)\n",
        "\n",
        "    # print(y_hat.size())\n",
        "    # print(y_hat)\n",
        "    # print(total_samples)\n",
        "    # print(y)\n",
        "\n",
        "  epoch_loss = running_loss / len(train_dataloader.dataset)\n",
        "  epoch_accuracy = total_correct / total_samples\n",
        "\n",
        "  return model, optimizer, epoch_loss, epoch_accuracy"
      ],
      "metadata": {
        "id": "K7AyEc_NqeuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define function to calculate validation loss and return\n",
        "# validation loss and validation epoch accuracy\n",
        "\n",
        "def calc_val_loss(val_dataloader, model, loss_func):\n",
        "\n",
        "  model.eval()\n",
        "  valid_loss = 0\n",
        "  total_correct = 0\n",
        "  total_samples = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, ((ecg, art, eeg), y) in enumerate(val_dataloader):\n",
        "\n",
        "      logits = model(ecg, art, eeg)\n",
        "      loss = loss_func(logits.view(logits.shape[0]), y)\n",
        "\n",
        "      valid_loss += (\n",
        "          (1 / (batch_idx + 1)) * (loss.data.item() - valid_loss)\n",
        "        )\n",
        "      # val_running_loss += loss.item() * y.size(0)\n",
        "\n",
        "      # calculate accuracy for current batch\n",
        "      y_hat = torch.sigmoid(model(ecg, art, eeg))\n",
        "      y_hat = y_hat.detach()\n",
        "      y_hat = (y_hat>0.5).int()\n",
        "      total_correct += (y_hat.view(y_hat.shape[0]) == y).sum().item()\n",
        "      total_samples += y.size(0)\n",
        "\n",
        "  # val_epoch_loss = val_running_loss / len(val_dataloader.dataset)\n",
        "  epoch_acc = total_correct / total_samples\n",
        "\n",
        "  return valid_loss, epoch_acc"
      ],
      "metadata": {
        "id": "CTj1doA6zh-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define functions to save and load model checkpoints\n",
        "\n",
        "def checkpoint(model, optimizer, filename):\n",
        "    torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            }, filename)\n",
        "\n",
        "def resume(model, optimizer, filename):\n",
        "\n",
        "  checkpoint = torch.load(filename)\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n"
      ],
      "metadata": {
        "id": "gDKh-PGp7C9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "\n",
        "def model_train(train_loader, val_loader, model, loss_func, optimizer, num_epoch, best_model_path, early_stop_thresh):\n",
        "\n",
        "  train_losses = []\n",
        "  val_losses = []\n",
        "  train_acc = []\n",
        "  val_acc = []\n",
        "\n",
        "  best_val_loss = 9999999\n",
        "  best_epoch = -1\n",
        "\n",
        "  for i in range(num_epoch):\n",
        "\n",
        "    model, optimizer, train_epoch_loss, train_epoch_acc = train_model_one_iter(train_loader, model, loss_func, optimizer)\n",
        "    print(f'Epoch: {i}, Train loss: {train_epoch_loss}, Train accuracy: {train_epoch_acc}')\n",
        "    train_losses.append(train_epoch_loss)\n",
        "    train_acc.append(train_epoch_acc)\n",
        "\n",
        "\n",
        "    val_epoch_loss, val_epoch_acc = calc_val_loss(val_loader, model, loss_func)\n",
        "    print(f'Val loss: {val_epoch_loss}, Val accuracy: {val_epoch_acc}')\n",
        "    val_losses.append(val_epoch_loss)\n",
        "    val_acc.append(val_epoch_acc)\n",
        "\n",
        "    if val_epoch_loss <= best_val_loss:\n",
        "        best_val_loss = val_epoch_loss\n",
        "        best_epoch = i\n",
        "        checkpoint(model, optimizer, best_model_path)\n",
        "    elif (i - best_epoch) > early_stop_thresh:\n",
        "        print(f'Early stopped training at epoch {i}')\n",
        "        break  # terminate the training loop\n",
        "\n",
        "    # if es.step(val_epoch_loss):\n",
        "    #   break\n",
        "\n",
        "  # show train and val losses\n",
        "  plt.figure(figsize=(10,5))\n",
        "  plt.title(\"Training and Validation Loss\")\n",
        "  plt.plot(val_losses,label=\"val\")\n",
        "  plt.plot(train_losses,label=\"train\")\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "QPJJjwCpqDY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Replicate paper: Train models for 3, 5, 10, and 15 min with Adam, lr=0.0001, and patience=5"
      ],
      "metadata": {
        "id": "3bOj-QozrBT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loss_func = nn.BCEWithLogitsLoss()\n",
        "# lr = 0.0001\n",
        "# num_epoch = 100\n",
        "# early_stop_thresh = 5\n",
        "\n",
        "# # 3 min model\n",
        "# torch.manual_seed(seed)\n",
        "# model = my_model()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "# best_model_path = \"/content/drive/MyDrive/VitalDB/best_model_3min.pth\"\n",
        "# model = model_train(train_loader_3min, val_loader_3min, model, loss_func, optimizer, num_epoch, best_model_path, early_stop_thresh)"
      ],
      "metadata": {
        "id": "I7IIILy7CCNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss_func = nn.BCEWithLogitsLoss()\n",
        "# lr = 0.0001\n",
        "# num_epoch = 100\n",
        "# early_stop_thresh = 5\n",
        "\n",
        "# # 5 min model\n",
        "# torch.manual_seed(seed)\n",
        "# model = my_model()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "# best_model_path = \"/content/drive/MyDrive/VitalDB/best_model_5min.pth\"\n",
        "# model = model_train(train_loader_5min, val_loader_5min, model, loss_func, optimizer, num_epoch, best_model_path, early_stop_thresh)"
      ],
      "metadata": {
        "id": "BLqoY0KSxFr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss_func = nn.BCEWithLogitsLoss()\n",
        "# lr = 0.0001\n",
        "# num_epoch = 100\n",
        "# early_stop_thresh = 5\n",
        "\n",
        "# # 10 min model\n",
        "# torch.manual_seed(seed)\n",
        "# model = my_model()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "# best_model_path = \"/content/drive/MyDrive/VitalDB/best_model_10min.pth\"\n",
        "# model = model_train(train_loader_10min, val_loader_10min, model, loss_func, optimizer, num_epoch, best_model_path, early_stop_thresh)"
      ],
      "metadata": {
        "id": "ur5mQgomxPbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss_func = nn.BCEWithLogitsLoss()\n",
        "# lr = 0.0001\n",
        "# num_epoch = 100\n",
        "# early_stop_thresh = 5\n",
        "\n",
        "# # 15 min model\n",
        "# torch.manual_seed(seed)\n",
        "# model = my_model()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "# best_model_path = \"/content/drive/MyDrive/VitalDB/best_model_15min.pth\"\n",
        "# model = model_train(train_loader_15min, val_loader_15min, model, loss_func, optimizer, num_epoch, best_model_path, early_stop_thresh)"
      ],
      "metadata": {
        "id": "AopL_xD6xd2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Search\n",
        "\n",
        "Test different batch sizes (128, 64, and 32) and learning rates (0.0001 and 0.001)"
      ],
      "metadata": {
        "id": "ULSI1tTuySTF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### batch size 128, learning rate of 0.001"
      ],
      "metadata": {
        "id": "poE9NbnfxlH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loss_func = nn.BCEWithLogitsLoss()\n",
        "# lr = 0.001\n",
        "# num_epoch = 100\n",
        "# early_stop_thresh = 5\n",
        "\n",
        "# # 3 min model\n",
        "# torch.manual_seed(seed)\n",
        "# model = my_model()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "# best_model_path = \"/content/drive/MyDrive/VitalDB/best_model_3min_adam_lr001.pth\"\n",
        "# model = model_train(train_loader_3min, val_loader_3min, model, loss_func, optimizer, num_epoch, best_model_path, early_stop_thresh)"
      ],
      "metadata": {
        "id": "2k38FErBxtNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss_func = nn.BCEWithLogitsLoss()\n",
        "# lr = 0.001\n",
        "# num_epoch = 100\n",
        "# early_stop_thresh = 5\n",
        "\n",
        "# # 5 min model\n",
        "# torch.manual_seed(seed)\n",
        "# model = my_model()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "# best_model_path = \"/content/drive/MyDrive/VitalDB/best_model_5min_adam_lr001.pth\"\n",
        "# model = model_train(train_loader_5min, val_loader_5min, model, loss_func, optimizer, num_epoch, best_model_path, early_stop_thresh)"
      ],
      "metadata": {
        "id": "bFskQ2DIEe3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss_func = nn.BCEWithLogitsLoss()\n",
        "# lr = 0.001\n",
        "# num_epoch = 100\n",
        "# early_stop_thresh = 5\n",
        "\n",
        "# # 10 min model\n",
        "# torch.manual_seed(seed)\n",
        "# model = my_model()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "# best_model_path = \"/content/drive/MyDrive/VitalDB/best_model_10min_adam_lr001.pth\"\n",
        "# model = model_train(train_loader_10min, val_loader_10min, model, loss_func, optimizer, num_epoch, best_model_path, early_stop_thresh)"
      ],
      "metadata": {
        "id": "AUQCu_z8EfjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss_func = nn.BCEWithLogitsLoss()\n",
        "# lr = 0.001\n",
        "# num_epoch = 100\n",
        "# early_stop_thresh = 5\n",
        "\n",
        "# # 15 min model\n",
        "# torch.manual_seed(seed)\n",
        "# model = my_model()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "# best_model_path = \"/content/drive/MyDrive/VitalDB/best_model_15min_adam_lr001.pth\"\n",
        "# model = model_train(train_loader_15min, val_loader_15min, model, loss_func, optimizer, num_epoch, best_model_path, early_stop_thresh)"
      ],
      "metadata": {
        "id": "HL7fuZQ0EgI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### batch size of 64, learning rate of 0.0001"
      ],
      "metadata": {
        "id": "V5BXdNQUyU6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train_loader_3min = load_data(HypoDataset(ecg_3min_train, art_3min_train, eeg_3min_train, y_3min_train), batch_size=64)\n",
        "# val_loader_3min = load_data(HypoDataset(ecg_3min_val, art_3min_val, eeg_3min_val, y_3min_val), batch_size=64)\n",
        "# test_loader_3min = load_data(HypoDataset(ecg_3min_test, art_3min_test, eeg_3min_test, y_3min_test), batch_size=64)\n",
        "\n",
        "# train_loader_5min = load_data(HypoDataset(ecg_5min_train, art_5min_train, eeg_5min_train, y_5min_train), batch_size=64)\n",
        "# val_loader_5min = load_data(HypoDataset(ecg_5min_val, art_5min_val, eeg_5min_val, y_5min_val), batch_size=64)\n",
        "# test_loader_5min = load_data(HypoDataset(ecg_5min_test, art_5min_test, eeg_5min_test, y_5min_test), batch_size=64)\n",
        "\n",
        "# train_loader_10min = load_data(HypoDataset(ecg_10min_train, art_10min_train, eeg_10min_train, y_10min_train), batch_size=64)\n",
        "# val_loader_10min = load_data(HypoDataset(ecg_10min_val, art_10min_val, eeg_10min_val, y_10min_val), batch_size=64)\n",
        "# test_loader_10min = load_data(HypoDataset(ecg_10min_test, art_10min_test, eeg_10min_test, y_10min_test), batch_size=64)\n",
        "\n",
        "# train_loader_15min = load_data(HypoDataset(ecg_15min_train, art_15min_train, eeg_15min_train, y_15min_train), batch_size=64)\n",
        "# val_loader_15min = load_data(HypoDataset(ecg_15min_val, art_15min_val, eeg_15min_val, y_15min_val), batch_size=64)\n",
        "# test_loader_15min = load_data(HypoDataset(ecg_15min_test, art_15min_test, eeg_15min_test, y_15min_test), batch_size=64)"
      ],
      "metadata": {
        "id": "HEnmIwXwzAmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss_func = nn.BCEWithLogitsLoss()\n",
        "# lr = 0.0001\n",
        "# num_epoch = 100\n",
        "# early_stop_thresh = 5\n",
        "\n",
        "# # 3 min model\n",
        "# torch.manual_seed(seed)\n",
        "# model = my_model()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "# best_model_path = \"/content/drive/MyDrive/VitalDB/best_model_3min_batch_64.pth\"\n",
        "# model = model_train(train_loader_3min, val_loader_3min, model, loss_func, optimizer, num_epoch, best_model_path, early_stop_thresh)"
      ],
      "metadata": {
        "id": "GCR1ErJsygm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss_func = nn.BCEWithLogitsLoss()\n",
        "# lr = 0.0001\n",
        "# num_epoch = 100\n",
        "# early_stop_thresh = 5\n",
        "\n",
        "# # 5 min model\n",
        "# torch.manual_seed(seed)\n",
        "# model = my_model()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "# best_model_path = \"/content/drive/MyDrive/VitalDB/best_model_5min_batch_64.pth\"\n",
        "# model = model_train(train_loader_5min, val_loader_5min, model, loss_func, optimizer, num_epoch, best_model_path, early_stop_thresh)"
      ],
      "metadata": {
        "id": "SWofkq6YUKcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss_func = nn.BCEWithLogitsLoss()\n",
        "# lr = 0.0001\n",
        "# num_epoch = 100\n",
        "# early_stop_thresh = 5\n",
        "\n",
        "# # 10 min model\n",
        "# torch.manual_seed(seed)\n",
        "# model = my_model()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "# best_model_path = \"/content/drive/MyDrive/VitalDB/best_model_10min_batch_64.pth\"\n",
        "# model = model_train(train_loader_10min, val_loader_10min, model, loss_func, optimizer, num_epoch, best_model_path, early_stop_thresh)"
      ],
      "metadata": {
        "id": "zzvMKRcmUK1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss_func = nn.BCEWithLogitsLoss()\n",
        "# lr = 0.0001\n",
        "# num_epoch = 100\n",
        "# early_stop_thresh = 5\n",
        "\n",
        "# # 15 min model\n",
        "# torch.manual_seed(seed)\n",
        "# model = my_model()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "# best_model_path = \"/content/drive/MyDrive/VitalDB/best_model_15min_batch_64.pth\"\n",
        "# model = model_train(train_loader_15min, val_loader_15min, model, loss_func, optimizer, num_epoch, best_model_path, early_stop_thresh)"
      ],
      "metadata": {
        "id": "RH4M3ytfULL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### batch size of 32, learning rate of 0.0001"
      ],
      "metadata": {
        "id": "J_xdbg-uzK5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train_loader_3min = load_data(HypoDataset(ecg_3min_train, art_3min_train, eeg_3min_train, y_3min_train), batch_size=32)\n",
        "# val_loader_3min = load_data(HypoDataset(ecg_3min_val, art_3min_val, eeg_3min_val, y_3min_val), batch_size=32)\n",
        "# test_loader_3min = load_data(HypoDataset(ecg_3min_test, art_3min_test, eeg_3min_test, y_3min_test), batch_size=32)\n",
        "\n",
        "# train_loader_5min = load_data(HypoDataset(ecg_5min_train, art_5min_train, eeg_5min_train, y_5min_train), batch_size=32)\n",
        "# val_loader_5min = load_data(HypoDataset(ecg_5min_val, art_5min_val, eeg_5min_val, y_5min_val), batch_size=32)\n",
        "# test_loader_5min = load_data(HypoDataset(ecg_5min_test, art_5min_test, eeg_5min_test, y_5min_test), batch_size=32)\n",
        "\n",
        "# train_loader_10min = load_data(HypoDataset(ecg_10min_train, art_10min_train, eeg_10min_train, y_10min_train), batch_size=32)\n",
        "# val_loader_10min = load_data(HypoDataset(ecg_10min_val, art_10min_val, eeg_10min_val, y_10min_val), batch_size=32)\n",
        "# test_loader_10min = load_data(HypoDataset(ecg_10min_test, art_10min_test, eeg_10min_test, y_10min_test), batch_size=32)\n",
        "\n",
        "# train_loader_15min = load_data(HypoDataset(ecg_15min_train, art_15min_train, eeg_15min_train, y_15min_train), batch_size=32)\n",
        "# val_loader_15min = load_data(HypoDataset(ecg_15min_val, art_15min_val, eeg_15min_val, y_15min_val), batch_size=32)\n",
        "# test_loader_15min = load_data(HypoDataset(ecg_15min_test, art_15min_test, eeg_15min_test, y_15min_test), batch_size=32)"
      ],
      "metadata": {
        "id": "yZE5Aa2pzbHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss_func = nn.BCEWithLogitsLoss()\n",
        "# lr = 0.0001\n",
        "# num_epoch = 100\n",
        "# early_stop_thresh = 5\n",
        "\n",
        "# # 3 min model\n",
        "# torch.manual_seed(seed)\n",
        "# model = my_model()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "# best_model_path = \"/content/drive/MyDrive/VitalDB/best_model_3min_batch_32.pth\"\n",
        "# model = model_train(train_loader_3min, val_loader_3min, model, loss_func, optimizer, num_epoch, best_model_path, early_stop_thresh)"
      ],
      "metadata": {
        "id": "i_JcjhQ9zNoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss_func = nn.BCEWithLogitsLoss()\n",
        "# lr = 0.0001\n",
        "# num_epoch = 100\n",
        "# early_stop_thresh = 5\n",
        "\n",
        "# # 5 min model\n",
        "# torch.manual_seed(seed)\n",
        "# model = my_model()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "# best_model_path = \"/content/drive/MyDrive/VitalDB/best_model_5min_batch_32.pth\"\n",
        "# model = model_train(train_loader_5min, val_loader_5min, model, loss_func, optimizer, num_epoch, best_model_path, early_stop_thresh)"
      ],
      "metadata": {
        "id": "P6bozZu9pOT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss_func = nn.BCEWithLogitsLoss()\n",
        "# lr = 0.0001\n",
        "# num_epoch = 100\n",
        "# early_stop_thresh = 5\n",
        "\n",
        "# # 10 min model\n",
        "# torch.manual_seed(seed)\n",
        "# model = my_model()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "# best_model_path = \"/content/drive/MyDrive/VitalDB/best_model_10min_batch_32.pth\"\n",
        "# model = model_train(train_loader_10min, val_loader_10min, model, loss_func, optimizer, num_epoch, best_model_path, early_stop_thresh)"
      ],
      "metadata": {
        "id": "3XCPpBJDpPNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss_func = nn.BCEWithLogitsLoss()\n",
        "# lr = 0.0001\n",
        "# num_epoch = 100\n",
        "# early_stop_thresh = 5\n",
        "\n",
        "# # 15 min model\n",
        "# torch.manual_seed(seed)\n",
        "# model = my_model()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "# best_model_path = \"/content/drive/MyDrive/VitalDB/best_model_15min_batch_32.pth\"\n",
        "# model = model_train(train_loader_15min, val_loader_15min, model, loss_func, optimizer, num_epoch, best_model_path, early_stop_thresh)"
      ],
      "metadata": {
        "id": "W44M9yjfpP6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### batch size of 64, learning rate of 0.001"
      ],
      "metadata": {
        "id": "FHzEAHKaiH4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train_loader_3min = load_data(HypoDataset(ecg_3min_train, art_3min_train, eeg_3min_train, y_3min_train), batch_size=64)\n",
        "# val_loader_3min = load_data(HypoDataset(ecg_3min_val, art_3min_val, eeg_3min_val, y_3min_val), batch_size=64)\n",
        "# test_loader_3min = load_data(HypoDataset(ecg_3min_test, art_3min_test, eeg_3min_test, y_3min_test), batch_size=64)\n",
        "\n",
        "# train_loader_5min = load_data(HypoDataset(ecg_5min_train, art_5min_train, eeg_5min_train, y_5min_train), batch_size=64)\n",
        "# val_loader_5min = load_data(HypoDataset(ecg_5min_val, art_5min_val, eeg_5min_val, y_5min_val), batch_size=64)\n",
        "# test_loader_5min = load_data(HypoDataset(ecg_5min_test, art_5min_test, eeg_5min_test, y_5min_test), batch_size=64)\n",
        "\n",
        "# train_loader_10min = load_data(HypoDataset(ecg_10min_train, art_10min_train, eeg_10min_train, y_10min_train), batch_size=64)\n",
        "# val_loader_10min = load_data(HypoDataset(ecg_10min_val, art_10min_val, eeg_10min_val, y_10min_val), batch_size=64)\n",
        "# test_loader_10min = load_data(HypoDataset(ecg_10min_test, art_10min_test, eeg_10min_test, y_10min_test), batch_size=64)\n",
        "\n",
        "# train_loader_15min = load_data(HypoDataset(ecg_15min_train, art_15min_train, eeg_15min_train, y_15min_train), batch_size=64)\n",
        "# val_loader_15min = load_data(HypoDataset(ecg_15min_val, art_15min_val, eeg_15min_val, y_15min_val), batch_size=64)\n",
        "# test_loader_15min = load_data(HypoDataset(ecg_15min_test, art_15min_test, eeg_15min_test, y_15min_test), batch_size=64)"
      ],
      "metadata": {
        "id": "HbzlK3NXIuCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss_func = nn.BCEWithLogitsLoss()\n",
        "# lr = 0.001\n",
        "# num_epoch = 100\n",
        "# early_stop_thresh = 5\n",
        "\n",
        "# # 3 min model\n",
        "# torch.manual_seed(seed)\n",
        "# model = my_model()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "# best_model_path = \"/content/drive/MyDrive/VitalDB/best_model_3min_batch_64_lr001.pth\"\n",
        "# model = model_train(train_loader_3min, val_loader_3min, model, loss_func, optimizer, num_epoch, best_model_path, early_stop_thresh)"
      ],
      "metadata": {
        "id": "on2gF3AsiX0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss_func = nn.BCEWithLogitsLoss()\n",
        "# lr = 0.001\n",
        "# num_epoch = 100\n",
        "# early_stop_thresh = 5\n",
        "\n",
        "# # 5 min model\n",
        "# torch.manual_seed(seed)\n",
        "# model = my_model()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "# best_model_path = \"/content/drive/MyDrive/VitalDB/best_model_5min_batch_64_lr001.pth\"\n",
        "# model = model_train(train_loader_5min, val_loader_5min, model, loss_func, optimizer, num_epoch, best_model_path, early_stop_thresh)"
      ],
      "metadata": {
        "id": "HU2c0w2kigrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss_func = nn.BCEWithLogitsLoss()\n",
        "# lr = 0.001\n",
        "# num_epoch = 100\n",
        "# early_stop_thresh = 5\n",
        "\n",
        "# # 10 min model\n",
        "# torch.manual_seed(seed)\n",
        "# model = my_model()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "# best_model_path = \"/content/drive/MyDrive/VitalDB/best_model_10min_batch_64_lr001.pth\"\n",
        "# model = model_train(train_loader_10min, val_loader_10min, model, loss_func, optimizer, num_epoch, best_model_path, early_stop_thresh)"
      ],
      "metadata": {
        "id": "vWsC6n6JikVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss_func = nn.BCEWithLogitsLoss()\n",
        "# lr = 0.001\n",
        "# num_epoch = 100\n",
        "# early_stop_thresh = 5\n",
        "\n",
        "# # 15 min model\n",
        "# torch.manual_seed(seed)\n",
        "# model = my_model()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "# best_model_path = \"/content/drive/MyDrive/VitalDB/best_model_15min_batch_64_lr001.pth\"\n",
        "# model = model_train(train_loader_15min, val_loader_15min, model, loss_func, optimizer, num_epoch, best_model_path, early_stop_thresh)"
      ],
      "metadata": {
        "id": "tthMmwPOioSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### batch size of 32, learning rate of 0.001"
      ],
      "metadata": {
        "id": "PLHmllphiyc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train_loader_3min = load_data(HypoDataset(ecg_3min_train, art_3min_train, eeg_3min_train, y_3min_train), batch_size=32)\n",
        "# val_loader_3min = load_data(HypoDataset(ecg_3min_val, art_3min_val, eeg_3min_val, y_3min_val), batch_size=32)\n",
        "# test_loader_3min = load_data(HypoDataset(ecg_3min_test, art_3min_test, eeg_3min_test, y_3min_test), batch_size=32)\n",
        "\n",
        "# train_loader_5min = load_data(HypoDataset(ecg_5min_train, art_5min_train, eeg_5min_train, y_5min_train), batch_size=32)\n",
        "# val_loader_5min = load_data(HypoDataset(ecg_5min_val, art_5min_val, eeg_5min_val, y_5min_val), batch_size=32)\n",
        "# test_loader_5min = load_data(HypoDataset(ecg_5min_test, art_5min_test, eeg_5min_test, y_5min_test), batch_size=32)\n",
        "\n",
        "# train_loader_10min = load_data(HypoDataset(ecg_10min_train, art_10min_train, eeg_10min_train, y_10min_train), batch_size=32)\n",
        "# val_loader_10min = load_data(HypoDataset(ecg_10min_val, art_10min_val, eeg_10min_val, y_10min_val), batch_size=32)\n",
        "# test_loader_10min = load_data(HypoDataset(ecg_10min_test, art_10min_test, eeg_10min_test, y_10min_test), batch_size=32)\n",
        "\n",
        "# train_loader_15min = load_data(HypoDataset(ecg_15min_train, art_15min_train, eeg_15min_train, y_15min_train), batch_size=32)\n",
        "# val_loader_15min = load_data(HypoDataset(ecg_15min_val, art_15min_val, eeg_15min_val, y_15min_val), batch_size=32)\n",
        "# test_loader_15min = load_data(HypoDataset(ecg_15min_test, art_15min_test, eeg_15min_test, y_15min_test), batch_size=32)"
      ],
      "metadata": {
        "id": "AEZUhPQ7i4tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss_func = nn.BCEWithLogitsLoss()\n",
        "# lr = 0.001\n",
        "# num_epoch = 100\n",
        "# early_stop_thresh = 5\n",
        "\n",
        "# # 3 min model\n",
        "# torch.manual_seed(seed)\n",
        "# model = my_model()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "# best_model_path = \"/content/drive/MyDrive/VitalDB/best_model_3min_batch_32_lr001.pth\"\n",
        "# model = model_train(train_loader_3min, val_loader_3min, model, loss_func, optimizer, num_epoch, best_model_path, early_stop_thresh)"
      ],
      "metadata": {
        "id": "-MmxbXkmjD9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss_func = nn.BCEWithLogitsLoss()\n",
        "# lr = 0.001\n",
        "# num_epoch = 100\n",
        "# early_stop_thresh = 5\n",
        "\n",
        "# # 5 min model\n",
        "# torch.manual_seed(seed)\n",
        "# model = my_model()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "# best_model_path = \"/content/drive/MyDrive/VitalDB/best_model_5min_batch_32_lr001.pth\"\n",
        "# model = model_train(train_loader_5min, val_loader_5min, model, loss_func, optimizer, num_epoch, best_model_path, early_stop_thresh)"
      ],
      "metadata": {
        "id": "fl3r52-ujExc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss_func = nn.BCEWithLogitsLoss()\n",
        "# lr = 0.001\n",
        "# num_epoch = 100\n",
        "# early_stop_thresh = 5\n",
        "\n",
        "# # 10 min model\n",
        "# torch.manual_seed(seed)\n",
        "# model = my_model()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "# best_model_path = \"/content/drive/MyDrive/VitalDB/best_model_10min_batch_32_lr001.pth\"\n",
        "# model = model_train(train_loader_10min, val_loader_10min, model, loss_func, optimizer, num_epoch, best_model_path, early_stop_thresh)"
      ],
      "metadata": {
        "id": "Wsy0mv3AjFrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss_func = nn.BCEWithLogitsLoss()\n",
        "# lr = 0.001\n",
        "# num_epoch = 100\n",
        "# early_stop_thresh = 5\n",
        "\n",
        "# # 15 min model\n",
        "# torch.manual_seed(seed)\n",
        "# model = my_model()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "# best_model_path = \"/content/drive/MyDrive/VitalDB/best_model_15min_batch_32_lr001.pth\"\n",
        "# model = model_train(train_loader_15min, val_loader_15min, model, loss_func, optimizer, num_epoch, best_model_path, early_stop_thresh)"
      ],
      "metadata": {
        "id": "zz6TcGNpjGv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ablations\n",
        "\n",
        "### Given the best performance with a batch size of 32 and a lr of 0.001 with the current dataset, testing of ablations with these settings:"
      ],
      "metadata": {
        "id": "eZzwBEChzQwE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader_3min = load_data(HypoDataset(ecg_3min_train, art_3min_train, eeg_3min_train, y_3min_train), batch_size=32)\n",
        "val_loader_3min = load_data(HypoDataset(ecg_3min_val, art_3min_val, eeg_3min_val, y_3min_val), batch_size=32)\n",
        "test_loader_3min = load_data(HypoDataset(ecg_3min_test, art_3min_test, eeg_3min_test, y_3min_test), batch_size=32)\n",
        "\n",
        "train_loader_5min = load_data(HypoDataset(ecg_5min_train, art_5min_train, eeg_5min_train, y_5min_train), batch_size=32)\n",
        "val_loader_5min = load_data(HypoDataset(ecg_5min_val, art_5min_val, eeg_5min_val, y_5min_val), batch_size=32)\n",
        "test_loader_5min = load_data(HypoDataset(ecg_5min_test, art_5min_test, eeg_5min_test, y_5min_test), batch_size=32)\n",
        "\n",
        "train_loader_10min = load_data(HypoDataset(ecg_10min_train, art_10min_train, eeg_10min_train, y_10min_train), batch_size=32)\n",
        "val_loader_10min = load_data(HypoDataset(ecg_10min_val, art_10min_val, eeg_10min_val, y_10min_val), batch_size=32)\n",
        "test_loader_10min = load_data(HypoDataset(ecg_10min_test, art_10min_test, eeg_10min_test, y_10min_test), batch_size=32)\n",
        "\n",
        "train_loader_15min = load_data(HypoDataset(ecg_15min_train, art_15min_train, eeg_15min_train, y_15min_train), batch_size=32)\n",
        "val_loader_15min = load_data(HypoDataset(ecg_15min_val, art_15min_val, eeg_15min_val, y_15min_val), batch_size=32)\n",
        "test_loader_15min = load_data(HypoDataset(ecg_15min_test, art_15min_test, eeg_15min_test, y_15min_test), batch_size=32)"
      ],
      "metadata": {
        "id": "zxMHbImHAuzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test model limited to ECG data"
      ],
      "metadata": {
        "id": "05gRKFjezXRW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ecg_model(nn.Module):\n",
        "  # use this class to define your model\n",
        "  def __init__(self):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder_ecg = nn.Sequential(\n",
        "                        nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer1_ecg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(1),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(1,2,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_ecg_layer1 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer2_ecg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer3_ecg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_ecg_layer3 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer4_ecg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer5_ecg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_ecg_layer5 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer6_ecg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,4,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(4,4,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer7_ecg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(4,4,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(4,4,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_ecg_layer7 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer8_ecg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(4,4,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(4,4,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer9_ecg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(4,4,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(4,4,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_ecg_layer9 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer10_ecg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(4,6,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(6,6,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer11_ecg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(6,6,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(6,6,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_ecg_layer11 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer12_ecg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(6,6,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.ReLU(),\n",
        "                        )\n",
        "\n",
        "    self.linear_ecg = nn.Linear(468*6, 96)\n",
        "\n",
        "    self.linear_combined1 = nn.Linear(96, 16)\n",
        "    self.linear_combined2 = nn.Linear(16, 1)\n",
        "\n",
        "    self.apply(self._init_weights)\n",
        "\n",
        "  def _init_weights(self, module):\n",
        "\n",
        "      if isinstance(module, nn.Conv1d):\n",
        "        nn.init.kaiming_normal_(module.weight, nonlinearity='relu')\n",
        "\n",
        "        if module.bias is not None:\n",
        "          nn.init.zeros_(module.bias)\n",
        "\n",
        "      elif isinstance(module, nn.BatchNorm1d):\n",
        "        nn.init.constant_(module.weight, 1)\n",
        "        nn.init.constant_(module.bias, 0)\n",
        "\n",
        "      elif isinstance(module, nn.Linear):\n",
        "        nn.init.xavier_normal_(module.weight)\n",
        "\n",
        "        if module.bias is not None:\n",
        "          nn.init.zeros_(module.bias)\n",
        "\n",
        "  def forward(self, ecg, art, eeg):\n",
        "\n",
        "    ecg = self.encoder_ecg(ecg)\n",
        "    tmp = self.layer1_ecg(ecg)\n",
        "    ecg = tmp + ecg\n",
        "    ecg = self.maxpool_ecg_layer1(ecg)\n",
        "    tmp = self.layer2_ecg(ecg)\n",
        "    ecg = tmp + ecg\n",
        "    tmp = self.layer3_ecg(ecg)\n",
        "    ecg = tmp + ecg\n",
        "    ecg = self.maxpool_ecg_layer3(ecg)\n",
        "    tmp = self.layer4_ecg(ecg)\n",
        "    ecg = tmp + ecg\n",
        "    tmp = self.layer5_ecg(ecg)\n",
        "    ecg = tmp + ecg\n",
        "    ecg = self.maxpool_ecg_layer5(ecg)\n",
        "    ecg = self.layer6_ecg(ecg)\n",
        "    tmp = self.layer7_ecg(ecg)\n",
        "    ecg = tmp + ecg\n",
        "    ecg = self.maxpool_ecg_layer7(ecg)\n",
        "    tmp = self.layer8_ecg(ecg)\n",
        "    ecg = tmp + ecg\n",
        "    tmp = self.layer9_ecg(ecg)\n",
        "    ecg = tmp + ecg\n",
        "    ecg = self.maxpool_ecg_layer9(ecg)\n",
        "    ecg = self.layer10_ecg(ecg)\n",
        "    tmp = self.layer11_ecg(ecg)\n",
        "    ecg = tmp + ecg\n",
        "    ecg = self.maxpool_ecg_layer11(ecg)\n",
        "    tmp = self.layer12_ecg(ecg)\n",
        "    ecg = tmp + ecg\n",
        "    ecg = torch.flatten(ecg, 1)\n",
        "    ecg = F.relu(self.linear_ecg(ecg))\n",
        "\n",
        "    combined = F.relu(self.linear_combined1(ecg))\n",
        "    logits = self.linear_combined2(combined)\n",
        "    # probs = F.sigmoid(logits)\n",
        "\n",
        "    return logits"
      ],
      "metadata": {
        "id": "i6VT-Lns0wgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss_func = nn.BCEWithLogitsLoss()\n",
        "# lr = 0.001\n",
        "# num_epoch = 100\n",
        "# early_stop_thresh = 5\n",
        "\n",
        "# # 3 min model\n",
        "# torch.manual_seed(seed)\n",
        "# model = ecg_model()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "# best_model_path = \"/content/drive/MyDrive/VitalDB/best_model_3min_ecg.pth\"\n",
        "# model = model_train(train_loader_3min, val_loader_3min, model, loss_func, optimizer, num_epoch, best_model_path, early_stop_thresh)"
      ],
      "metadata": {
        "id": "liN79kpgzedB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss_func = nn.BCEWithLogitsLoss()\n",
        "# lr = 0.001\n",
        "# num_epoch = 100\n",
        "# early_stop_thresh = 5\n",
        "\n",
        "# # 5 min model\n",
        "# torch.manual_seed(seed)\n",
        "# model = ecg_model()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "# best_model_path = \"/content/drive/MyDrive/VitalDB/best_model_5min_ecg.pth\"\n",
        "# model = model_train(train_loader_5min, val_loader_5min, model, loss_func, optimizer, num_epoch, best_model_path, early_stop_thresh)"
      ],
      "metadata": {
        "id": "Q6dIazMQBM_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss_func = nn.BCEWithLogitsLoss()\n",
        "# lr = 0.001\n",
        "# num_epoch = 100\n",
        "# early_stop_thresh = 5\n",
        "\n",
        "# # 10 min model\n",
        "# torch.manual_seed(seed)\n",
        "# model = ecg_model()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "# best_model_path = \"/content/drive/MyDrive/VitalDB/best_model_10min_ecg.pth\"\n",
        "# model = model_train(train_loader_10min, val_loader_10min, model, loss_func, optimizer, num_epoch, best_model_path, early_stop_thresh)"
      ],
      "metadata": {
        "id": "IuYB78PBBOAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss_func = nn.BCEWithLogitsLoss()\n",
        "# lr = 0.001\n",
        "# num_epoch = 100\n",
        "# early_stop_thresh = 5\n",
        "\n",
        "# # 15 min model\n",
        "# torch.manual_seed(seed)\n",
        "# model = ecg_model()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "# best_model_path = \"/content/drive/MyDrive/VitalDB/best_model_15min_ecg.pth\"\n",
        "# model = model_train(train_loader_15min, val_loader_15min, model, loss_func, optimizer, num_epoch, best_model_path, early_stop_thresh)"
      ],
      "metadata": {
        "id": "sL_99aKYBPKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test model limited to ART (blood pressure) data"
      ],
      "metadata": {
        "id": "iar6E_LBzlkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class art_model(nn.Module):\n",
        "  # use this class to define your model\n",
        "  def __init__(self):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder_art = nn.Sequential(\n",
        "                        nn.Conv1d(1,1,3,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer1_art = nn.Sequential(\n",
        "                        nn.BatchNorm1d(1),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(1,2,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_art_layer1 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer2_art = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer3_art = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_art_layer3 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer4_art = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer5_art = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_art_layer5 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer6_art = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,4,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(4,4,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer7_art = nn.Sequential(\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(4,4,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(4,4,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_art_layer7 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer8_art = nn.Sequential(\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(4,4,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(4,4,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer9_art = nn.Sequential(\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(4,4,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(4,4,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_art_layer9 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer10_art = nn.Sequential(\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(4,6,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(6,6,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer11_art = nn.Sequential(\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(6,6,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(6,6,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_art_layer11 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer12_art = nn.Sequential(\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(6,6,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.ReLU(),\n",
        "                        )\n",
        "\n",
        "    self.linear_art = nn.Linear(468*6, 96)\n",
        "\n",
        "    self.linear_combined1 = nn.Linear(96, 16)\n",
        "    self.linear_combined2 = nn.Linear(16, 1)\n",
        "\n",
        "    self.apply(self._init_weights)\n",
        "\n",
        "  def _init_weights(self, module):\n",
        "\n",
        "      if isinstance(module, nn.Conv1d):\n",
        "        nn.init.kaiming_normal_(module.weight, nonlinearity='relu')\n",
        "\n",
        "        if module.bias is not None:\n",
        "          nn.init.zeros_(module.bias)\n",
        "\n",
        "      elif isinstance(module, nn.BatchNorm1d):\n",
        "        nn.init.constant_(module.weight, 1)\n",
        "        nn.init.constant_(module.bias, 0)\n",
        "\n",
        "      elif isinstance(module, nn.Linear):\n",
        "        nn.init.xavier_normal_(module.weight)\n",
        "\n",
        "        if module.bias is not None:\n",
        "          nn.init.zeros_(module.bias)\n",
        "\n",
        "  def forward(self, ecg, art, eeg):\n",
        "\n",
        "    art = self.encoder_art(art)\n",
        "    tmp = self.layer1_art(art)\n",
        "    art = tmp + art\n",
        "    art = self.maxpool_art_layer1(art)\n",
        "    tmp = self.layer2_art(art)\n",
        "    art = tmp + art\n",
        "    tmp = self.layer3_art(art)\n",
        "    art = tmp + art\n",
        "    art = self.maxpool_art_layer3(art)\n",
        "    tmp = self.layer4_art(art)\n",
        "    art = tmp + art\n",
        "    tmp = self.layer5_art(art)\n",
        "    art = tmp + art\n",
        "    art = self.maxpool_art_layer5(art)\n",
        "    art = self.layer6_art(art)\n",
        "    tmp = self.layer7_art(art)\n",
        "    art = tmp + art\n",
        "    art = self.maxpool_art_layer7(art)\n",
        "    tmp = self.layer8_art(art)\n",
        "    art = tmp + art\n",
        "    tmp = self.layer9_art(art)\n",
        "    art = tmp + art\n",
        "    art = self.maxpool_art_layer9(art)\n",
        "    art = self.layer10_art(art)\n",
        "    tmp = self.layer11_art(art)\n",
        "    art = tmp + art\n",
        "    art = self.maxpool_art_layer11(art)\n",
        "    tmp = self.layer12_art(art)\n",
        "    art = tmp + art\n",
        "    art = torch.flatten(art, 1)\n",
        "    art = F.relu(self.linear_art(art))\n",
        "\n",
        "    combined = F.relu(self.linear_combined1(art))\n",
        "    logits = self.linear_combined2(combined)\n",
        "    # probs = F.sigmoid(logits)\n",
        "\n",
        "    return logits"
      ],
      "metadata": {
        "id": "Gg23aHuhNVA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss_func = nn.BCEWithLogitsLoss()\n",
        "# lr = 0.001\n",
        "# num_epoch = 100\n",
        "# early_stop_thresh = 5\n",
        "\n",
        "# # 3 min model\n",
        "# torch.manual_seed(seed)\n",
        "# model = art_model()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "# best_model_path = \"/content/drive/MyDrive/VitalDB/best_model_3min_art.pth\"\n",
        "# model = model_train(train_loader_3min, val_loader_3min, model, loss_func, optimizer, num_epoch, best_model_path, early_stop_thresh)"
      ],
      "metadata": {
        "id": "b8A-dPpWzqY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss_func = nn.BCEWithLogitsLoss()\n",
        "# lr = 0.001\n",
        "# num_epoch = 100\n",
        "# early_stop_thresh = 5\n",
        "\n",
        "# # 5 min model\n",
        "# torch.manual_seed(seed)\n",
        "# model = art_model()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "# best_model_path = \"/content/drive/MyDrive/VitalDB/best_model_5min_art.pth\"\n",
        "# model = model_train(train_loader_5min, val_loader_5min, model, loss_func, optimizer, num_epoch, best_model_path, early_stop_thresh)"
      ],
      "metadata": {
        "id": "pQdzDI4bBiLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss_func = nn.BCEWithLogitsLoss()\n",
        "# lr = 0.001\n",
        "# num_epoch = 100\n",
        "# early_stop_thresh = 5\n",
        "\n",
        "# # 10 min model\n",
        "# torch.manual_seed(seed)\n",
        "# model = art_model()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "# best_model_path = \"/content/drive/MyDrive/VitalDB/best_model_10min_art.pth\"\n",
        "# model = model_train(train_loader_10min, val_loader_10min, model, loss_func, optimizer, num_epoch, best_model_path, early_stop_thresh)"
      ],
      "metadata": {
        "id": "lE2qq7tVBjF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss_func = nn.BCEWithLogitsLoss()\n",
        "# lr = 0.001\n",
        "# num_epoch = 100\n",
        "# early_stop_thresh = 5\n",
        "\n",
        "# # 15 min model\n",
        "# torch.manual_seed(seed)\n",
        "# model = art_model()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "# best_model_path = \"/content/drive/MyDrive/VitalDB/best_model_15min_art.pth\"\n",
        "# model = model_train(train_loader_15min, val_loader_15min, model, loss_func, optimizer, num_epoch, best_model_path, early_stop_thresh)"
      ],
      "metadata": {
        "id": "K1m2mGjsBj9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test model limited to EEG data"
      ],
      "metadata": {
        "id": "u2Dc4tl4zu12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class eeg_model(nn.Module):\n",
        "  # use this class to define your model\n",
        "  def __init__(self):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder_eeg = nn.Sequential(\n",
        "                        nn.Conv1d(1,1,3,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer1_eeg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(1),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(1,2,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_eeg_layer1 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer2_eeg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer3_eeg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_eeg_layer3 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer4_eeg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer5_eeg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_eeg_layer5 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer6_eeg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,4,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(4,4,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer7_eeg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(4,4,3,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(4,4,3,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_eeg_layer7 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer8_eeg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(4,4,3,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(4,4,3,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer9_eeg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(4,4,3,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(4,4,3,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_eeg_layer9 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer10_eeg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(4,6,3,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(6,6,3,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer11_eeg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(6,6,3,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(6,6,3,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_eeg_layer11 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer12_eeg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(6,6,3,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.ReLU()\n",
        "                        )\n",
        "\n",
        "    self.linear_eeg = nn.Linear(120*6, 96)\n",
        "\n",
        "    self.linear_combined1 = nn.Linear(96, 16)\n",
        "    self.linear_combined2 = nn.Linear(16, 1)\n",
        "\n",
        "    self.apply(self._init_weights)\n",
        "\n",
        "  def _init_weights(self, module):\n",
        "\n",
        "      if isinstance(module, nn.Conv1d):\n",
        "        nn.init.kaiming_normal_(module.weight, nonlinearity='relu')\n",
        "\n",
        "        if module.bias is not None:\n",
        "          nn.init.zeros_(module.bias)\n",
        "\n",
        "      elif isinstance(module, nn.BatchNorm1d):\n",
        "        nn.init.constant_(module.weight, 1)\n",
        "        nn.init.constant_(module.bias, 0)\n",
        "\n",
        "      elif isinstance(module, nn.Linear):\n",
        "        nn.init.xavier_normal_(module.weight)\n",
        "\n",
        "        if module.bias is not None:\n",
        "          nn.init.zeros_(module.bias)\n",
        "\n",
        "  def forward(self, ecg, art, eeg):\n",
        "\n",
        "    eeg = self.encoder_eeg(eeg)\n",
        "    tmp = self.layer1_eeg(eeg)\n",
        "    eeg = tmp + eeg\n",
        "    eeg = self.maxpool_eeg_layer1(eeg)\n",
        "    tmp = self.layer2_eeg(eeg)\n",
        "    eeg = tmp + eeg\n",
        "    tmp = self.layer3_eeg(eeg)\n",
        "    eeg = tmp + eeg\n",
        "    eeg = self.maxpool_eeg_layer3(eeg)\n",
        "    tmp = self.layer4_eeg(eeg)\n",
        "    eeg = tmp + eeg\n",
        "    tmp = self.layer5_eeg(eeg)\n",
        "    eeg = tmp + eeg\n",
        "    eeg = self.maxpool_eeg_layer5(eeg)\n",
        "    eeg = self.layer6_eeg(eeg)\n",
        "    tmp = self.layer7_eeg(eeg)\n",
        "    eeg = tmp + eeg\n",
        "    eeg = self.maxpool_eeg_layer7(eeg)\n",
        "    tmp = self.layer8_eeg(eeg)\n",
        "    eeg = tmp + eeg\n",
        "    tmp = self.layer9_eeg(eeg)\n",
        "    eeg = tmp + eeg\n",
        "    eeg = self.maxpool_eeg_layer9(eeg)\n",
        "    eeg = self.layer10_eeg(eeg)\n",
        "    tmp = self.layer11_eeg(eeg)\n",
        "    eeg = tmp + eeg\n",
        "    eeg = self.maxpool_eeg_layer11(eeg)\n",
        "    tmp = self.layer12_eeg(eeg)\n",
        "    eeg = tmp + eeg\n",
        "    eeg = torch.flatten(eeg, 1)\n",
        "    eeg = F.relu(self.linear_eeg(eeg))\n",
        "\n",
        "    combined = F.relu(self.linear_combined1(eeg))\n",
        "    logits = self.linear_combined2(combined)\n",
        "    # probs = F.sigmoid(logits)\n",
        "\n",
        "    return logits"
      ],
      "metadata": {
        "id": "sm5lb38Z1Lvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss_func = nn.BCEWithLogitsLoss()\n",
        "# lr = 0.001\n",
        "# num_epoch = 100\n",
        "# early_stop_thresh = 5\n",
        "\n",
        "# # 3 min model\n",
        "# torch.manual_seed(seed)\n",
        "# model = eeg_model()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "# best_model_path = \"/content/drive/MyDrive/VitalDB/best_model_3min_eeg.pth\"\n",
        "# model = model_train(train_loader_3min, val_loader_3min, model, loss_func, optimizer, num_epoch, best_model_path, early_stop_thresh)"
      ],
      "metadata": {
        "id": "CoJhBB6PzxQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss_func = nn.BCEWithLogitsLoss()\n",
        "# lr = 0.001\n",
        "# num_epoch = 100\n",
        "# early_stop_thresh = 5\n",
        "\n",
        "# # 5 min model\n",
        "# torch.manual_seed(seed)\n",
        "# model = eeg_model()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "# best_model_path = \"/content/drive/MyDrive/VitalDB/best_model_5min_eeg.pth\"\n",
        "# model = model_train(train_loader_5min, val_loader_5min, model, loss_func, optimizer, num_epoch, best_model_path, early_stop_thresh)"
      ],
      "metadata": {
        "id": "vGERnehqB6Ms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss_func = nn.BCEWithLogitsLoss()\n",
        "# lr = 0.001\n",
        "# num_epoch = 100\n",
        "# early_stop_thresh = 5\n",
        "\n",
        "# # 10 min model\n",
        "# torch.manual_seed(seed)\n",
        "# model = eeg_model()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "# best_model_path = \"/content/drive/MyDrive/VitalDB/best_model_10min_eeg.pth\"\n",
        "# model = model_train(train_loader_10min, val_loader_10min, model, loss_func, optimizer, num_epoch, best_model_path, early_stop_thresh)"
      ],
      "metadata": {
        "id": "lUVTmFbRB648"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss_func = nn.BCEWithLogitsLoss()\n",
        "# lr = 0.001\n",
        "# num_epoch = 100\n",
        "# early_stop_thresh = 5\n",
        "\n",
        "# # 15 min model\n",
        "# torch.manual_seed(seed)\n",
        "# model = eeg_model()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "# best_model_path = \"/content/drive/MyDrive/VitalDB/best_model_15min_eeg.pth\"\n",
        "# model = model_train(train_loader_15min, val_loader_15min, model, loss_func, optimizer, num_epoch, best_model_path, early_stop_thresh)"
      ],
      "metadata": {
        "id": "2pdhHKLTB7y-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test model with only 6 residual blocks instead of 12"
      ],
      "metadata": {
        "id": "mxpMjlyPz3KZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class shallow_model(nn.Module):\n",
        "  # use this class to define your model\n",
        "  def __init__(self):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder_ecg = nn.Sequential(\n",
        "                        nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer1_ecg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(1),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(1,2,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_ecg_layer1 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer2_ecg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer3_ecg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_ecg_layer3 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer4_ecg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer5_ecg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_ecg_layer5 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer6_ecg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,4,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU()\n",
        "                        )\n",
        "\n",
        "    self.linear_ecg = nn.Linear(3750*4, 32)\n",
        "\n",
        "    self.encoder_art = nn.Sequential(\n",
        "                        nn.Conv1d(1,1,3,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer1_art = nn.Sequential(\n",
        "                        nn.BatchNorm1d(1),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(1,2,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_art_layer1 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer2_art = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer3_art = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_art_layer3 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer4_art = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer5_art = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_art_layer5 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer6_art = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,4,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU()\n",
        "                        )\n",
        "\n",
        "    self.linear_art = nn.Linear(3750*4, 32)\n",
        "\n",
        "    self.encoder_eeg = nn.Sequential(\n",
        "                        nn.Conv1d(1,1,3,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer1_eeg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(1),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(1,2,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_eeg_layer1 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer2_eeg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer3_eeg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_eeg_layer3 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer4_eeg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer5_eeg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Conv1d(2,2,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_eeg_layer5 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer6_eeg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.ReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,4,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.ReLU()\n",
        "                        )\n",
        "\n",
        "    self.linear_eeg = nn.Linear(960*4, 32)\n",
        "\n",
        "    self.linear_combined1 = nn.Linear(96, 16)\n",
        "    self.linear_combined2 = nn.Linear(16, 1)\n",
        "\n",
        "    self.apply(self._init_weights)\n",
        "\n",
        "  def _init_weights(self, module):\n",
        "\n",
        "      if isinstance(module, nn.Conv1d):\n",
        "        nn.init.kaiming_normal_(module.weight, nonlinearity='relu')\n",
        "\n",
        "        if module.bias is not None:\n",
        "          nn.init.zeros_(module.bias)\n",
        "\n",
        "      elif isinstance(module, nn.BatchNorm1d):\n",
        "        nn.init.constant_(module.weight, 1)\n",
        "        nn.init.constant_(module.bias, 0)\n",
        "\n",
        "      elif isinstance(module, nn.Linear):\n",
        "        nn.init.xavier_normal_(module.weight)\n",
        "\n",
        "        if module.bias is not None:\n",
        "          nn.init.zeros_(module.bias)\n",
        "\n",
        "  def forward(self, ecg, art, eeg):\n",
        "\n",
        "    ecg = self.encoder_ecg(ecg)\n",
        "    tmp = self.layer1_ecg(ecg)\n",
        "    ecg = tmp + ecg\n",
        "    ecg = self.maxpool_ecg_layer1(ecg)\n",
        "    tmp = self.layer2_ecg(ecg)\n",
        "    ecg = tmp + ecg\n",
        "    tmp = self.layer3_ecg(ecg)\n",
        "    ecg = tmp + ecg\n",
        "    ecg = self.maxpool_ecg_layer3(ecg)\n",
        "    tmp = self.layer4_ecg(ecg)\n",
        "    ecg = tmp + ecg\n",
        "    tmp = self.layer5_ecg(ecg)\n",
        "    ecg = tmp + ecg\n",
        "    ecg = self.maxpool_ecg_layer5(ecg)\n",
        "    ecg = self.layer6_ecg(ecg)\n",
        "    ecg = torch.flatten(ecg, 1)\n",
        "    ecg = F.relu(self.linear_ecg(ecg))\n",
        "\n",
        "    art = self.encoder_art(art)\n",
        "    tmp = self.layer1_art(art)\n",
        "    art = tmp + art\n",
        "    art = self.maxpool_art_layer1(art)\n",
        "    tmp = self.layer2_art(art)\n",
        "    art = tmp + art\n",
        "    tmp = self.layer3_art(art)\n",
        "    art = tmp + art\n",
        "    art = self.maxpool_art_layer3(art)\n",
        "    tmp = self.layer4_art(art)\n",
        "    art = tmp + art\n",
        "    tmp = self.layer5_art(art)\n",
        "    art = tmp + art\n",
        "    art = self.maxpool_art_layer5(art)\n",
        "    art = self.layer6_art(art)\n",
        "    art = torch.flatten(art, 1)\n",
        "    art = F.relu(self.linear_art(art))\n",
        "\n",
        "    eeg = self.encoder_eeg(eeg)\n",
        "    tmp = self.layer1_eeg(eeg)\n",
        "    eeg = tmp + eeg\n",
        "    eeg = self.maxpool_eeg_layer1(eeg)\n",
        "    tmp = self.layer2_eeg(eeg)\n",
        "    eeg = tmp + eeg\n",
        "    tmp = self.layer3_eeg(eeg)\n",
        "    eeg = tmp + eeg\n",
        "    eeg = self.maxpool_eeg_layer3(eeg)\n",
        "    tmp = self.layer4_eeg(eeg)\n",
        "    eeg = tmp + eeg\n",
        "    tmp = self.layer5_eeg(eeg)\n",
        "    eeg = tmp + eeg\n",
        "    eeg = self.maxpool_eeg_layer5(eeg)\n",
        "    eeg = self.layer6_eeg(eeg)\n",
        "    eeg = torch.flatten(eeg, 1)\n",
        "    eeg = F.relu(self.linear_eeg(eeg))\n",
        "\n",
        "    combined = F.relu(self.linear_combined1(torch.cat((ecg, art, eeg), -1)))\n",
        "    logits = self.linear_combined2(combined)\n",
        "    # probs = F.sigmoid(logits)\n",
        "\n",
        "    return logits"
      ],
      "metadata": {
        "id": "fnzWZvft1NPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss_func = nn.BCEWithLogitsLoss()\n",
        "# lr = 0.001\n",
        "# num_epoch = 100\n",
        "# early_stop_thresh = 5\n",
        "\n",
        "# # 3 min model\n",
        "# torch.manual_seed(seed)\n",
        "# model = shallow_model()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "# best_model_path = \"/content/drive/MyDrive/VitalDB/best_model_3min_shallow.pth\"\n",
        "# model = model_train(train_loader_3min, val_loader_3min, model, loss_func, optimizer, num_epoch, best_model_path, early_stop_thresh)"
      ],
      "metadata": {
        "id": "R1k-T0oIz8gk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss_func = nn.BCEWithLogitsLoss()\n",
        "# lr = 0.001\n",
        "# num_epoch = 100\n",
        "# early_stop_thresh = 5\n",
        "\n",
        "# # 5 min model\n",
        "# torch.manual_seed(seed)\n",
        "# model = shallow_model()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "# best_model_path = \"/content/drive/MyDrive/VitalDB/best_model_5min_shallow.pth\"\n",
        "# model = model_train(train_loader_5min, val_loader_5min, model, loss_func, optimizer, num_epoch, best_model_path, early_stop_thresh)"
      ],
      "metadata": {
        "id": "iVESb9wzCWj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss_func = nn.BCEWithLogitsLoss()\n",
        "# lr = 0.001\n",
        "# num_epoch = 100\n",
        "# early_stop_thresh = 5\n",
        "\n",
        "# # 10 min model\n",
        "# torch.manual_seed(seed)\n",
        "# model = shallow_model()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "# best_model_path = \"/content/drive/MyDrive/VitalDB/best_model_10min_shallow.pth\"\n",
        "# model = model_train(train_loader_10min, val_loader_10min, model, loss_func, optimizer, num_epoch, best_model_path, early_stop_thresh)"
      ],
      "metadata": {
        "id": "rsAtQ6ncCXUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss_func = nn.BCEWithLogitsLoss()\n",
        "# lr = 0.001\n",
        "# num_epoch = 100\n",
        "# early_stop_thresh = 5\n",
        "\n",
        "# # 15 min model\n",
        "# torch.manual_seed(seed)\n",
        "# model = shallow_model()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "# best_model_path = \"/content/drive/MyDrive/VitalDB/best_model_15min_shallow.pth\"\n",
        "# model = model_train(train_loader_15min, val_loader_15min, model, loss_func, optimizer, num_epoch, best_model_path, early_stop_thresh)"
      ],
      "metadata": {
        "id": "Uy6TUBOWCYD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing Leaky RELU instead of RELU"
      ],
      "metadata": {
        "id": "hmpU6tPo0Okr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class model_leaky_relu(nn.Module):\n",
        "  # use this class to define your model\n",
        "  def __init__(self):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder_ecg = nn.Sequential(\n",
        "                        nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer1_ecg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(1),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(1,2,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_ecg_layer1 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer2_ecg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer3_ecg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_ecg_layer3 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer4_ecg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer5_ecg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_ecg_layer5 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer6_ecg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,4,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Conv1d(4,4,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer7_ecg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(4,4,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Conv1d(4,4,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_ecg_layer7 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer8_ecg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(4,4,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Conv1d(4,4,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer9_ecg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(4,4,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Conv1d(4,4,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_ecg_layer9 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer10_ecg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(4,6,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Conv1d(6,6,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer11_ecg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(6,6,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Conv1d(6,6,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_ecg_layer11 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer12_ecg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(6,6,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.LeakyReLU(),\n",
        "                        )\n",
        "\n",
        "    self.linear_ecg = nn.Linear(468*6, 32)\n",
        "\n",
        "    self.encoder_art = nn.Sequential(\n",
        "                        nn.Conv1d(1,1,3,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer1_art = nn.Sequential(\n",
        "                        nn.BatchNorm1d(1),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(1,2,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_art_layer1 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer2_art = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer3_art = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_art_layer3 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer4_art = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer5_art = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Conv1d(2,2,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_art_layer5 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer6_art = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,4,15,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Conv1d(4,4,15,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer7_art = nn.Sequential(\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(4,4,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Conv1d(4,4,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_art_layer7 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer8_art = nn.Sequential(\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(4,4,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Conv1d(4,4,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer9_art = nn.Sequential(\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(4,4,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Conv1d(4,4,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_art_layer9 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer10_art = nn.Sequential(\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(4,6,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Conv1d(6,6,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer11_art = nn.Sequential(\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(6,6,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Conv1d(6,6,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_art_layer11 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer12_art = nn.Sequential(\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(6,6,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.LeakyReLU(),\n",
        "                        )\n",
        "\n",
        "    self.linear_art = nn.Linear(468*6, 32)\n",
        "\n",
        "    self.encoder_eeg = nn.Sequential(\n",
        "                        nn.Conv1d(1,1,3,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer1_eeg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(1),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(1,2,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Conv1d(2,2,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_eeg_layer1 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer2_eeg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Conv1d(2,2,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer3_eeg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Conv1d(2,2,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_eeg_layer3 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer4_eeg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Conv1d(2,2,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer5_eeg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,2,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Conv1d(2,2,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_eeg_layer5 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer6_eeg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(2),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(2,4,7,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Conv1d(4,4,7,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer7_eeg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(4,4,3,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Conv1d(4,4,3,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_eeg_layer7 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer8_eeg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(4,4,3,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Conv1d(4,4,3,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer9_eeg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(4,4,3,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Conv1d(4,4,3,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_eeg_layer9 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer10_eeg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(4),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(4,6,3,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Conv1d(6,6,3,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.layer11_eeg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(6,6,3,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Conv1d(6,6,3,stride=1, padding='same')\n",
        "                        )\n",
        "\n",
        "    self.maxpool_eeg_layer11 = nn.MaxPool1d(2, 2)\n",
        "\n",
        "    self.layer12_eeg = nn.Sequential(\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.LeakyReLU(),\n",
        "                        nn.Dropout(),\n",
        "                        nn.Conv1d(6,6,3,stride=1, padding='same'),\n",
        "                        nn.BatchNorm1d(6),\n",
        "                        nn.LeakyReLU(),\n",
        "                        )\n",
        "\n",
        "    self.linear_eeg = nn.Linear(120*6, 32)\n",
        "\n",
        "    self.linear_combined1 = nn.Linear(96, 16)\n",
        "    self.linear_combined2 = nn.Linear(16, 1)\n",
        "\n",
        "    self.apply(self._init_weights)\n",
        "\n",
        "  def _init_weights(self, module):\n",
        "\n",
        "      if isinstance(module, nn.Conv1d):\n",
        "        nn.init.kaiming_normal_(module.weight, nonlinearity='leaky_relu')\n",
        "\n",
        "        if module.bias is not None:\n",
        "          nn.init.zeros_(module.bias)\n",
        "\n",
        "      elif isinstance(module, nn.BatchNorm1d):\n",
        "        nn.init.constant_(module.weight, 1)\n",
        "        nn.init.constant_(module.bias, 0)\n",
        "\n",
        "      elif isinstance(module, nn.Linear):\n",
        "        nn.init.xavier_normal_(module.weight)\n",
        "\n",
        "        if module.bias is not None:\n",
        "          nn.init.zeros_(module.bias)\n",
        "\n",
        "  def forward(self, ecg, art, eeg):\n",
        "\n",
        "    ecg = self.encoder_ecg(ecg)\n",
        "    tmp = self.layer1_ecg(ecg)\n",
        "    ecg = tmp + ecg\n",
        "    ecg = self.maxpool_ecg_layer1(ecg)\n",
        "    tmp = self.layer2_ecg(ecg)\n",
        "    ecg = tmp + ecg\n",
        "    tmp = self.layer3_ecg(ecg)\n",
        "    ecg = tmp + ecg\n",
        "    ecg = self.maxpool_ecg_layer3(ecg)\n",
        "    tmp = self.layer4_ecg(ecg)\n",
        "    ecg = tmp + ecg\n",
        "    tmp = self.layer5_ecg(ecg)\n",
        "    ecg = tmp + ecg\n",
        "    ecg = self.maxpool_ecg_layer5(ecg)\n",
        "    ecg = self.layer6_ecg(ecg)\n",
        "    tmp = self.layer7_ecg(ecg)\n",
        "    ecg = tmp + ecg\n",
        "    ecg = self.maxpool_ecg_layer7(ecg)\n",
        "    tmp = self.layer8_ecg(ecg)\n",
        "    ecg = tmp + ecg\n",
        "    tmp = self.layer9_ecg(ecg)\n",
        "    ecg = tmp + ecg\n",
        "    ecg = self.maxpool_ecg_layer9(ecg)\n",
        "    ecg = self.layer10_ecg(ecg)\n",
        "    tmp = self.layer11_ecg(ecg)\n",
        "    ecg = tmp + ecg\n",
        "    ecg = self.maxpool_ecg_layer11(ecg)\n",
        "    tmp = self.layer12_ecg(ecg)\n",
        "    ecg = tmp + ecg\n",
        "    ecg = torch.flatten(ecg, 1)\n",
        "    ecg = F.leaky_relu(self.linear_ecg(ecg))\n",
        "\n",
        "    art = self.encoder_art(art)\n",
        "    tmp = self.layer1_art(art)\n",
        "    art = tmp + art\n",
        "    art = self.maxpool_art_layer1(art)\n",
        "    tmp = self.layer2_art(art)\n",
        "    art = tmp + art\n",
        "    tmp = self.layer3_art(art)\n",
        "    art = tmp + art\n",
        "    art = self.maxpool_art_layer3(art)\n",
        "    tmp = self.layer4_art(art)\n",
        "    art = tmp + art\n",
        "    tmp = self.layer5_art(art)\n",
        "    art = tmp + art\n",
        "    art = self.maxpool_art_layer5(art)\n",
        "    art = self.layer6_art(art)\n",
        "    tmp = self.layer7_art(art)\n",
        "    art = tmp + art\n",
        "    art = self.maxpool_art_layer7(art)\n",
        "    tmp = self.layer8_art(art)\n",
        "    art = tmp + art\n",
        "    tmp = self.layer9_art(art)\n",
        "    art = tmp + art\n",
        "    art = self.maxpool_art_layer9(art)\n",
        "    art = self.layer10_art(art)\n",
        "    tmp = self.layer11_art(art)\n",
        "    art = tmp + art\n",
        "    art = self.maxpool_art_layer11(art)\n",
        "    tmp = self.layer12_art(art)\n",
        "    art = tmp + art\n",
        "    art = torch.flatten(art, 1)\n",
        "    art = F.leaky_relu(self.linear_art(art))\n",
        "\n",
        "    eeg = self.encoder_eeg(eeg)\n",
        "    tmp = self.layer1_eeg(eeg)\n",
        "    eeg = tmp + eeg\n",
        "    eeg = self.maxpool_eeg_layer1(eeg)\n",
        "    tmp = self.layer2_eeg(eeg)\n",
        "    eeg = tmp + eeg\n",
        "    tmp = self.layer3_eeg(eeg)\n",
        "    eeg = tmp + eeg\n",
        "    eeg = self.maxpool_eeg_layer3(eeg)\n",
        "    tmp = self.layer4_eeg(eeg)\n",
        "    eeg = tmp + eeg\n",
        "    tmp = self.layer5_eeg(eeg)\n",
        "    eeg = tmp + eeg\n",
        "    eeg = self.maxpool_eeg_layer5(eeg)\n",
        "    eeg = self.layer6_eeg(eeg)\n",
        "    tmp = self.layer7_eeg(eeg)\n",
        "    eeg = tmp + eeg\n",
        "    eeg = self.maxpool_eeg_layer7(eeg)\n",
        "    tmp = self.layer8_eeg(eeg)\n",
        "    eeg = tmp + eeg\n",
        "    tmp = self.layer9_eeg(eeg)\n",
        "    eeg = tmp + eeg\n",
        "    eeg = self.maxpool_eeg_layer9(eeg)\n",
        "    eeg = self.layer10_eeg(eeg)\n",
        "    tmp = self.layer11_eeg(eeg)\n",
        "    eeg = tmp + eeg\n",
        "    eeg = self.maxpool_eeg_layer11(eeg)\n",
        "    tmp = self.layer12_eeg(eeg)\n",
        "    eeg = tmp + eeg\n",
        "    eeg = torch.flatten(eeg, 1)\n",
        "    eeg = F.leaky_relu(self.linear_eeg(eeg))\n",
        "\n",
        "    combined = F.leaky_relu(self.linear_combined1(torch.cat((ecg, art, eeg), -1)))\n",
        "    logits = self.linear_combined2(combined)\n",
        "    # probs = F.sigmoid(logits)\n",
        "\n",
        "    return logits"
      ],
      "metadata": {
        "id": "e5AbuIF70hwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss_func = nn.BCEWithLogitsLoss()\n",
        "# lr = 0.001\n",
        "# num_epoch = 100\n",
        "# early_stop_thresh = 5\n",
        "\n",
        "# # 3 min model\n",
        "# torch.manual_seed(seed)\n",
        "# model = model_leaky_relu()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "# best_model_path = \"/content/drive/MyDrive/VitalDB/best_model_3min_leaky_relu.pth\"\n",
        "# model = model_train(train_loader_3min, val_loader_3min, model, loss_func, optimizer, num_epoch, best_model_path, early_stop_thresh)"
      ],
      "metadata": {
        "id": "Hbxp3r7y21m5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss_func = nn.BCEWithLogitsLoss()\n",
        "# lr = 0.001\n",
        "# num_epoch = 100\n",
        "# early_stop_thresh = 5\n",
        "\n",
        "# # 5 min model\n",
        "# torch.manual_seed(seed)\n",
        "# model = model_leaky_relu()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "# best_model_path = \"/content/drive/MyDrive/VitalDB/best_model_5min_leaky_relu.pth\"\n",
        "# model = model_train(train_loader_5min, val_loader_5min, model, loss_func, optimizer, num_epoch, best_model_path, early_stop_thresh)"
      ],
      "metadata": {
        "id": "VYyGNmdh3y3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss_func = nn.BCEWithLogitsLoss()\n",
        "# lr = 0.001\n",
        "# num_epoch = 100\n",
        "# early_stop_thresh = 5\n",
        "\n",
        "# # 10 min model\n",
        "# torch.manual_seed(seed)\n",
        "# model = model_leaky_relu()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "# best_model_path = \"/content/drive/MyDrive/VitalDB/best_model_10min_leaky_relu.pth\"\n",
        "# model = model_train(train_loader_10min, val_loader_10min, model, loss_func, optimizer, num_epoch, best_model_path, early_stop_thresh)"
      ],
      "metadata": {
        "id": "jbDXduHm34R7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss_func = nn.BCEWithLogitsLoss()\n",
        "# lr = 0.001\n",
        "# num_epoch = 100\n",
        "# early_stop_thresh = 5\n",
        "\n",
        "# # 15 min model\n",
        "# torch.manual_seed(seed)\n",
        "# model = model_leaky_relu()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "# best_model_path = \"/content/drive/MyDrive/VitalDB/best_model_15min_leaky_relu.pth\"\n",
        "# model = model_train(train_loader_15min, val_loader_15min, model, loss_func, optimizer, num_epoch, best_model_path, early_stop_thresh)"
      ],
      "metadata": {
        "id": "eyyTDpv639j_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "In this section, you should finish training your model training or loading your trained model. That is a great experiment! You should share the results with others with necessary metrics and figures.\n",
        "\n",
        "Please test and report results for all experiments that you run with:\n",
        "\n",
        "*   specific numbers (accuracy, AUC, RMSE, etc)\n",
        "*   figures (loss shrinkage, outputs from GAN, annotation or label of sample pictures, etc)\n"
      ],
      "metadata": {
        "id": "gX6bCcZNuxmz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Evaluation**"
      ],
      "metadata": {
        "id": "iHsT7oNgq1Ij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model(test_dataloader, model):\n",
        "\n",
        "    model.eval()\n",
        "    Y_score = []\n",
        "    Y_pred = []\n",
        "    Y_true = []\n",
        "\n",
        "    for (ecg, art, eeg), y in test_dataloader:\n",
        "\n",
        "        y_hat = torch.sigmoid(model(ecg, art, eeg))\n",
        "        y_hat = y_hat.detach()\n",
        "        Y_score.append(y_hat)\n",
        "\n",
        "        y_hat = (y_hat>0.5).int()\n",
        "        Y_pred.append(y_hat)\n",
        "\n",
        "        Y_true.append(y)\n",
        "\n",
        "    Y_score = np.concatenate(Y_score, axis=0)\n",
        "    Y_pred = np.concatenate(Y_pred, axis=0)\n",
        "    Y_true = np.concatenate(Y_true, axis=0)\n",
        "\n",
        "    return Y_score, Y_pred, Y_true"
      ],
      "metadata": {
        "id": "jqKj15x4-dUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_eval_parameters(test_dataloader, model_architecture, lr, optimizer, PATH):\n",
        "\n",
        "  model = model_architecture\n",
        "  lr = lr\n",
        "  optimizer = optimizer(model.parameters(), lr = lr)\n",
        "\n",
        "  resume(model, optimizer, PATH)\n",
        "\n",
        "  y_score, y_pred, y_true = eval_model(test_dataloader, model)\n",
        "\n",
        "  # calculate result metrics\n",
        "  acc = accuracy_score(y_true, y_pred)\n",
        "  roc_auc = roc_auc_score(y_true, y_score)\n",
        "  pr_auc = average_precision_score(y_true, y_score)\n",
        "\n",
        "  print(f'Test Accuracy: {acc}')\n",
        "  print(f'Test AUC: {roc_auc}')\n",
        "  print(f'Test PR AUC: {pr_auc}')\n",
        "\n",
        "  # prepare score array for plotting\n",
        "  y_score_oth_class = np.ones_like(y_score.T[0]) - y_score.T[0]\n",
        "  y_score_comb = np.stack((y_score_oth_class, y_score.T[0]), axis=1)\n",
        "\n",
        "  # Plot AUC\n",
        "  skplt.plotters.plot_roc_curve(y_true, y_score_comb, curves=['macro'])\n",
        "  plt.show()\n",
        "\n",
        "  # Plot precision - recall curve\n",
        "  precision, recall, thresholds = precision_recall_curve(y_true, y_score)\n",
        "  plt.plot(recall, precision)\n",
        "  plt.title('Precision Recall Curve')\n",
        "  plt.xlabel('Recall')\n",
        "  plt.ylabel('Precision')\n",
        "  plt.show()\n",
        "\n",
        "  return acc, roc_auc, pr_auc, y_true, y_score_comb"
      ],
      "metadata": {
        "id": "dhl3DQRQDrRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline model with best performance (batch=32 and lr=0.001)"
      ],
      "metadata": {
        "id": "iKgKiwxt4nQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 minute model\n",
        "acc_3min_batch32_lr001, roc_auc_3min_batch32_lr001, pr_auc_3min_batch32_lr001, y_true_3min_batch32_lr001, y_score_comb_3min_batch32_lr001 = \\\n",
        "print_eval_parameters(test_loader_3min, my_model(), 0.001, torch.optim.Adam,\n",
        "                      \"/content/drive/MyDrive/VitalDB/best_model_3min_batch_32_lr001.pth\")"
      ],
      "metadata": {
        "id": "uoCYskaRGTPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 minute mo\n",
        "acc_5min_batch32_lr001, roc_auc_5min_batch32_lr001, pr_auc_5min_batch32_lr001, y_true_5min_batch32_lr001, y_score_comb_5min_batch32_lr001 = \\\n",
        "print_eval_parameters(test_loader_5min, my_model(), 0.001, torch.optim.Adam,\n",
        "                      \"/content/drive/MyDrive/VitalDB/best_model_5min_batch_32_lr001.pth\")"
      ],
      "metadata": {
        "id": "3-DZpI-45lPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10 minute model\n",
        "acc_10min_batch32_lr001, roc_auc_10min_batch32_lr001, pr_auc_10min_batch32_lr001, y_true_10min_batch32_lr001, y_score_comb_10min_batch32_lr001 = \\\n",
        "print_eval_parameters(test_loader_10min, my_model(), 0.001, torch.optim.Adam,\n",
        "                      \"/content/drive/MyDrive/VitalDB/best_model_10min_batch_32_lr001.pth\")"
      ],
      "metadata": {
        "id": "z4pqV4Xu5l0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 15 minute model\n",
        "acc_15min_batch32_lr001, roc_auc_15min_batch32_lr001, pr_auc_15min_batch32_lr001, y_true_15min_batch32_lr001, y_score_comb_15min_batch32_lr001 = \\\n",
        "print_eval_parameters(test_loader_15min, my_model(), 0.001, torch.optim.Adam,\n",
        "                      \"/content/drive/MyDrive/VitalDB/best_model_15min_batch_32_lr001.pth\")"
      ],
      "metadata": {
        "id": "uhdYFt8L5mgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test effect of ablations"
      ],
      "metadata": {
        "id": "IFMFRua5Hr3R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ECG only**"
      ],
      "metadata": {
        "id": "Q1aoFIXwH6IW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 minute model\n",
        "acc_3min_ecg, roc_auc_3min_ecg, pr_auc_3min_ecg, y_true_3min_ecg, y_score_comb_3min_ecg = \\\n",
        "print_eval_parameters(test_loader_3min, ecg_model(), 0.001, torch.optim.Adam,\n",
        "                      \"/content/drive/MyDrive/VitalDB/best_model_3min_ecg.pth\")"
      ],
      "metadata": {
        "id": "e6qQC6voH9Dj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 minute model\n",
        "acc_5min_ecg, roc_auc_5min_ecg, pr_auc_5min_ecg, y_true_5min_ecg, y_score_comb_5min_ecg = \\\n",
        "print_eval_parameters(test_loader_5min, ecg_model(), 0.001, torch.optim.Adam,\n",
        "                      \"/content/drive/MyDrive/VitalDB/best_model_5min_ecg.pth\")"
      ],
      "metadata": {
        "id": "gesCvOz8IkLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10 minute model\n",
        "acc_10min_ecg, roc_auc_10min_ecg, pr_auc_10min_ecg, y_true_10min_ecg, y_score_comb_10min_ecg = \\\n",
        "print_eval_parameters(test_loader_10min, ecg_model(), 0.001, torch.optim.Adam,\n",
        "                      \"/content/drive/MyDrive/VitalDB/best_model_10min_ecg.pth\")"
      ],
      "metadata": {
        "id": "i4m_pG9UIrmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 15 minute model\n",
        "acc_3min_ecg, roc_auc_15min_ecg, pr_auc_15min_ecg, y_true_15min_ecg, y_score_comb_15min_ecg = \\\n",
        "print_eval_parameters(test_loader_15min, ecg_model(), 0.001, torch.optim.Adam,\n",
        "                      \"/content/drive/MyDrive/VitalDB/best_model_15min_ecg.pth\")"
      ],
      "metadata": {
        "id": "TFR7MIuqI16Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ART model**"
      ],
      "metadata": {
        "id": "fCrSuX0NI_ZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 minute model\n",
        "acc_3min_art, roc_auc_3min_art, pr_auc_3min_art, y_true_3min_art, y_score_comb_3min_art = \\\n",
        "print_eval_parameters(test_loader_3min, art_model(), 0.001, torch.optim.Adam,\n",
        "                      \"/content/drive/MyDrive/VitalDB/best_model_3min_art.pth\")"
      ],
      "metadata": {
        "id": "vHZ3CjTiJDQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 minute model\n",
        "acc_5min_art, roc_auc_5min_art, pr_auc_5min_art, y_true_5min_art, y_score_comb_5min_art = \\\n",
        "print_eval_parameters(test_loader_5min, art_model(), 0.001, torch.optim.Adam,\n",
        "                      \"/content/drive/MyDrive/VitalDB/best_model_5min_art.pth\")"
      ],
      "metadata": {
        "id": "8aa3XBvyJPVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10 minute model\n",
        "acc_10min_art, roc_auc_10min_art, pr_auc_10min_art, y_true_10min_art, y_score_comb_10min_art = \\\n",
        "print_eval_parameters(test_loader_10min, art_model(), 0.001, torch.optim.Adam,\n",
        "                      \"/content/drive/MyDrive/VitalDB/best_model_10min_art.pth\")"
      ],
      "metadata": {
        "id": "Mnj-u62kJZYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 15 minute model\n",
        "acc_15min_art, roc_auc_15min_art, pr_auc_15min_art, y_true_15min_art, y_score_comb_15min_art = \\\n",
        "print_eval_parameters(test_loader_15min, art_model(), 0.001, torch.optim.Adam,\n",
        "                      \"/content/drive/MyDrive/VitalDB/best_model_15min_art.pth\")"
      ],
      "metadata": {
        "id": "Ut-XdVlgJjqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EEG model**"
      ],
      "metadata": {
        "id": "e9EbUFctJ7F1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 minute model\n",
        "acc_3min_eeg, roc_auc_3min_eeg, pr_auc_3min_eeg, y_true_3min_eeg, y_score_comb_3min_eeg = \\\n",
        "print_eval_parameters(test_loader_3min, eeg_model(), 0.001, torch.optim.Adam,\n",
        "                      \"/content/drive/MyDrive/VitalDB/best_model_3min_eeg.pth\")"
      ],
      "metadata": {
        "id": "HuhAUYE8J-GW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 minute model\n",
        "acc_5min_eeg, roc_auc_5min_eeg, pr_auc_5min_eeg, y_true_5min_eeg, y_score_comb_5min_eeg = \\\n",
        "print_eval_parameters(test_loader_5min, eeg_model(), 0.001, torch.optim.Adam,\n",
        "                      \"/content/drive/MyDrive/VitalDB/best_model_5min_eeg.pth\")"
      ],
      "metadata": {
        "id": "096X7stXKLee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10 minute model\n",
        "acc_10min_eeg, roc_auc_10min_eeg, pr_auc_10min_eeg, y_true_10min_eeg, y_score_comb_10min_eeg = \\\n",
        "print_eval_parameters(test_loader_10min, eeg_model(), 0.001, torch.optim.Adam,\n",
        "                      \"/content/drive/MyDrive/VitalDB/best_model_10min_eeg.pth\")"
      ],
      "metadata": {
        "id": "U-oiH_dTKaOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 15 minute model\n",
        "acc_15min_eeg, roc_auc_15min_eeg, pr_auc_15min_eeg, y_true_15min_eeg, y_score_comb_15min_eeg = \\\n",
        "print_eval_parameters(test_loader_15min, eeg_model(), 0.001, torch.optim.Adam,\n",
        "                      \"/content/drive/MyDrive/VitalDB/best_model_15min_eeg.pth\")"
      ],
      "metadata": {
        "id": "v9jaiMb3Kpdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Shallow model**"
      ],
      "metadata": {
        "id": "Y2cqt1V-K1BY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 minute model\n",
        "acc_3min_shallow, roc_auc_3min_shallow, pr_auc_3min_shallow, y_true_3min_shallow, y_score_comb_3min_shallow = \\\n",
        "print_eval_parameters(test_loader_3min, shallow_model(), 0.001, torch.optim.Adam,\n",
        "                      \"/content/drive/MyDrive/VitalDB/best_model_3min_shallow.pth\")"
      ],
      "metadata": {
        "id": "G5dD2mVwK4J7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 minute model\n",
        "acc_5min_shallow, roc_auc_5min_shallow, pr_auc_5min_shallow, y_true_5min_shallow, y_score_comb_5min_shallow = \\\n",
        "print_eval_parameters(test_loader_5min, shallow_model(), 0.001, torch.optim.Adam,\n",
        "                      \"/content/drive/MyDrive/VitalDB/best_model_5min_shallow.pth\")"
      ],
      "metadata": {
        "id": "y7gZ9VX8LHLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10 minute model\n",
        "acc_10min_shallow, roc_auc_10min_shallow, pr_auc_10min_shallow, y_true_10min_shallow, y_score_comb_10min_shallow = \\\n",
        "print_eval_parameters(test_loader_10min, shallow_model(), 0.001, torch.optim.Adam,\n",
        "                      \"/content/drive/MyDrive/VitalDB/best_model_10min_shallow.pth\")"
      ],
      "metadata": {
        "id": "m4y8HODnLIzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 15 minute model\n",
        "acc_15min_shallow, roc_auc_15min_shallow, pr_auc_15min_shallow, y_true_15min_shallow, y_score_comb_15min_shallow = \\\n",
        "print_eval_parameters(test_loader_15min, shallow_model(), 0.001, torch.optim.Adam,\n",
        "                      \"/content/drive/MyDrive/VitalDB/best_model_15min_shallow.pth\")"
      ],
      "metadata": {
        "id": "UvsVopMQLMta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model with leaky Relu activation**"
      ],
      "metadata": {
        "id": "SF1zr81rLr7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 minute model\n",
        "acc_3min_leaky_relu, roc_auc_3min_leaky_relu, pr_auc_3min_leaky_relu, y_true_3min_leaky_relu, y_score_comb_3min_leaky_relu = \\\n",
        "print_eval_parameters(test_loader_3min, model_leaky_relu(), 0.001, torch.optim.Adam,\n",
        "                      \"/content/drive/MyDrive/VitalDB/best_model_3min_leaky_relu.pth\")"
      ],
      "metadata": {
        "id": "ikrcsCriLxxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 minute model\n",
        "acc_5min_leaky_relu, roc_auc_5min_leaky_relu, pr_auc_5min_leaky_relu, y_true_5min_leaky_relu, y_score_comb_5min_leaky_relu = \\\n",
        "print_eval_parameters(test_loader_5min, model_leaky_relu(), 0.001, torch.optim.Adam,\n",
        "                      \"/content/drive/MyDrive/VitalDB/best_model_5min_leaky_relu.pth\")"
      ],
      "metadata": {
        "id": "Pbw1bhn3MKL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10 minute model\n",
        "acc_10min_leaky_relu, roc_auc_10min_leaky_relu, pr_auc_10min_leaky_relu, y_true_10min_leaky_relu, y_score_comb_10min_leaky_relu = \\\n",
        "print_eval_parameters(test_loader_10min, model_leaky_relu(), 0.001, torch.optim.Adam,\n",
        "                      \"/content/drive/MyDrive/VitalDB/best_model_10min_leaky_relu.pth\")"
      ],
      "metadata": {
        "id": "5MdpTlFyML02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 15 minute model\n",
        "acc_15min_leaky_relu, roc_auc_15min_leaky_relu, pr_auc_15min_leaky_relu, y_true_15min_leaky_relu, y_score_comb_15min_leaky_relu = \\\n",
        "print_eval_parameters(test_loader_15min, model_leaky_relu(), 0.001, torch.optim.Adam,\n",
        "                      \"/content/drive/MyDrive/VitalDB/best_model_15min_leaky_relu.pth\")"
      ],
      "metadata": {
        "id": "khtl2POuMOmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model comparison"
      ],
      "metadata": {
        "id": "8EAWAy_LwHlV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compare you model with others\n",
        "# you don't need to re-run all other experiments, instead, you can directly refer the metrics/numbers in the paper"
      ],
      "metadata": {
        "id": "uOdhGrbwwG71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*1) Compare main model to model originally published*\n",
        "\n",
        "The results of the main analysis (model combining ECG, ART, and EEG) are slightly worse than the performance of the model in the original paper:\n",
        "\n",
        "  3 min:\n",
        "- Original AUROC: 0.957, original AUPRC: 0.926\n",
        "- Reproduced AUROC: 0.852, reproduced AUPRC: 0.915\n",
        "\n",
        "  5 min:\n",
        "- Original AUROC: 0.926, original AUPRC: 0.867\n",
        "- Reproduced AUROC: 0.800, reproduced AUPRC: 0.878\n",
        "\n",
        "  10 min:\n",
        "- Original AUROC: 0.895, original AUPRC: 0.817\n",
        "- Reproduced AUROC: 0.756, reproduced AUPRC: 0.840\n",
        "\n",
        "  15 min:\n",
        "- Original AUROC: 0.868, original AUPRC: 0.778\n",
        "- Reproduced AUROC: 0.761, reproduced AUPRC: 0.843\n",
        "\n",
        "Accuracy was not reported in the original paper\n",
        "\n",
        "\n",
        "*2) Compare ECG only models*\n",
        "\n",
        "  3 min:\n",
        "- Original AUROC: 0.634, original AUPRC: 0.339\n",
        "- Reproduced AUROC: 0.483, reproduced AUPRC: 0.579\n",
        "\n",
        "  5 min:\n",
        "- Original AUROC: 0.652, original AUPRC: 0.359\n",
        "- Reproduced AUROC: 0.521, reproduced AUPRC: 0.606\n",
        "\n",
        "  10 min:\n",
        "- Original AUROC: 0.659, original AUPRC: 0.364\n",
        "- Reproduced AUROC: 0.472, reproduced AUPRC: 0.553\n",
        "\n",
        "  15 min:\n",
        "- Original AUROC: 0.640, original AUPRC: 0.306\n",
        "- Reproduced AUROC: 0.495, reproduced AUPRC: 0.572\n",
        "\n",
        "\n",
        "*3) Compare ART only models*\n",
        "\n",
        "  3 min:\n",
        "- Original AUROC: 0.968, original AUPRC: 0.939\n",
        "- Reproduced AUROC: 0.823, reproduced AUPRC: 0.876\n",
        "\n",
        "  5 min:\n",
        "- Original AUROC: 0.930, original AUPRC: 0.873\n",
        "- Reproduced AUROC: 0.733, reproduced AUPRC: 0.770\n",
        "\n",
        "  10 min:\n",
        "- Original AUROC: 0.892, original AUPRC: 0.814\n",
        "- Reproduced AUROC: 0.493, reproduced AUPRC: 0.609\n",
        "\n",
        "  15 min:\n",
        "- Original AUROC: 0.889, original AUPRC: 0.803\n",
        "- Reproduced AUROC: 0.501, reproduced AUPRC: 0.579\n",
        "\n",
        "\n",
        "*4) Compare EEG only models*\n",
        "\n",
        "  3 min:\n",
        "- Original AUROC: 0.557, original AUPRC: 0.286\n",
        "- Reproduced AUROC: 0.489, reproduced AUPRC: 0.589\n",
        "\n",
        "  5 min:\n",
        "- Original AUROC: 0.581, original AUPRC: 0.301\n",
        "- Reproduced AUROC: 0.502, reproduced AUPRC: 0.589\n",
        "\n",
        "  10 min:\n",
        "- Original AUROC: 0.584, original AUPRC: 0.297\n",
        "- Reproduced AUROC: 0.496, reproduced AUPRC: 0.573\n",
        "\n",
        "  15 min:\n",
        "- Reproduced AUROC: 0.577, reproduced AUPRC: 0.260\n",
        "- Reproduced AUROC: 0.501, reproduced AUPRC: 0.579\n",
        "\n",
        "\n",
        "*5) Shallow model (not evaluated in original paper):*\n",
        "\n",
        "  3 min:\n",
        "- Reproduced AUROC: 0.850, reproduced AUPRC: 0.899\n",
        "\n",
        "  5 min:\n",
        "- Reproduced AUROC: 0.809, reproduced AUPRC: 0.879\n",
        "\n",
        "  10 min:\n",
        "- Reproduced AUROC: 0.745, reproduced AUPRC: 0.838\n",
        "\n",
        "  15 min:\n",
        "- Reproduced AUROC: 0.749, reproduced AUPRC: 0.832\n",
        "\n",
        "\n",
        "*6) Model with leaky Relu activations (not evaluated in original paper):*\n",
        "\n",
        "  3 min:\n",
        "- Reproduced AUROC: 0.859, reproduced AUPRC: 0.922\n",
        "\n",
        "  5 min:\n",
        "- Reproduced AUROC: 0.806, reproduced AUPRC: 0.881\n",
        "\n",
        "  10 min:\n",
        "- Reproduced AUROC: 0.755, reproduced AUPRC: 0.840\n",
        "\n",
        "  15 min:\n",
        "- Reproduced AUROC: 0.766, reproduced AUPRC: 0.848"
      ],
      "metadata": {
        "id": "iOMyQ-hhM48f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion\n",
        "\n",
        "The main takeaways from this analysis are:\n",
        "\n",
        "- The reproduced metrics are somewhat worse than the ones originally published. Possible reasons include:\n",
        "    - The larger dataset used in the original analysis (N=120,416 cases and controls across the training, validation, and test datasets vs 8,903 cases and controls used for the current analysis\n",
        "    - Exclusion of patients with poor arterial blood pressure tracings (defined as a jSQI<0.8) in the original study. The jSQI was not evaluated here as the signal analysis code is written in Matlab, which cannot be easily incorporated in Colab. However, tracings with very low or very high blood pressures as defined in the jSQI algorithm were excluded from the current analysis\n",
        "\n",
        "- Similar to findings in the original study, models trained based on 3 minute intervals prior to a hypotensive events consistently performed better than models trained based on tracings recorded 5, 10, or 15 min before a hypotensive event. This makes physiologic sense considering that it is much easier to predicts complications arising shortly after a measurment was obtained\n",
        "\n",
        "- As opposed to the original paper, a learning rate of 0.001 seemed to lead to a better performance than a learning rate of 0.0001, possibly due to the different number of samples included or due to variations in other hyperparameters such as batch size which were not mentioned in the original paper\n",
        "\n",
        "- As mentioned above, several important details and hyperparameters are lacking in the original paper. These include the batch size (n=32 seemed to be best based on the current hyperparameter search) and weight initialization (Kaiming and Xavier were chosen here based on preliminary tests comparing them to default initializations; data not shown)\n",
        "\n",
        "- Arterial blood pressure appeared to be the most important tracing to predict subsequent hypotensive events. Ablation studies demonstrated that models only based on ECG or EEG data are very difficult or nearly impossible to train (the training performed for the current analysis demonstrates overfitting of the training dataset for ECG and EEG tracings, likely related to the very deep model and the relatively few samples included here. this is supported by a slightly better performance [similar to what was seen in the original paper] when using a more shallow model for this ablation [data not shown]). Physiologically, it is expected that low blood pressure predicts subsequent low blood pressure events and that the ECG and especially EEG (which is a noisy tracing or brain activity) do not correlate with future hypotensive events\n",
        "\n",
        "- The performance of the shallow model with only half as many layers is basically equivalent to the original model. However, it might be possible that the performance of the deeper model can be improved more significantly with a much larger dataset\n",
        "\n",
        "- There was essentially no difference between using RELU and leaky RELU activations"
      ],
      "metadata": {
        "id": "qH75TNU71eRH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "1. \tJo Y-Y, Jang J-H, Kwon J, Lee H-C, Jung C-W, Byun S, Jeong H-G. Predicting intraoperative hypotension using deep learning with waveforms of arterial blood pressure, electroencephalogram, and electrocardiogram: Retrospective study. PLOS ONE 2022;17:e0272055. doi: 10.1371/journal.pone.0272055\n",
        "2. \tSalmasi V, Maheshwari K, Yang D, Mascha EJ, Singh A, Sessler DI, Kurz A. Relationship between Intraoperative Hypotension, Defined by Either Reduction from Baseline or Absolute Thresholds, and Acute Kidney and Myocardial Injury after Noncardiac Surgery: A Retrospective Cohort Analysis. Anesthesiology 2017;126:47–65. doi: 10.1097/ALN.0000000000001432\n",
        "3. \tLi Q, Mark RG, Clifford GD. Artificial arterial blood pressure artifact models and an evaluation of a robust blood pressure and heart rate estimator. Biomed Eng Online 2009;8:13. doi: 10.1186/1475-925X-8-13.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SHMI2chl9omn"
      }
    }
  ]
}